{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the VCF file you wish to analyze\n",
    "import os\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "inputVCFFileName=list(uploaded.keys())[0]\n",
    "outBase=inputVCFFileName.replace('.vcf','')\n",
    "os.rename(inputVCFFileName, 'input.vcf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this VCF file in GRCh37 or GRCh38 coordinates? \n",
    "genome='GRCh37'\n",
    "# uncomment the below line if using GRCh38\n",
    "#genome='GRCh38'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget pandas numpy matplotlib scikit-learn scipy biopython\n",
    "!pip install tensorflow==2.7\n",
    "!pip install tf-models-official==2.7\n",
    "!pip install transformers\n",
    "# download the resources\n",
    "!python -m wget https://zuchnerlab.s3.amazonaws.com/VariantPathogenicity/Maverick_resources.tar.gz\n",
    "!tar -zxvf Maverick_resources.tar.gz\n",
    "!rm Maverick_resources.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# process variants with annovar\n",
    "echo \"Starting Step 1: Get coding changes with Annovar\"\n",
    "dos2unix input.vcf\n",
    "grep -v '^#' input.vcf | cut -f 1,2,4,5 > input_locations.txt\n",
    "annovar/convert2annovar.pl -format vcf4 input.vcf > input.avinput\n",
    "if [[ {genome} == 'GRCh37' ]]; then\n",
    "    annovar/annotate_variation.pl -dbtype wgEncodeGencodeBasicV33lift37 -buildver hg19 --exonicsplicing input.avinput annovar/humandb/\n",
    "else\n",
    "    annovar/annotate_variation.pl -dbtype wgEncodeGencodeBasicV33 -buildver hg38 --exonicsplicing input.avinput annovar/humandb/\n",
    "fi\n",
    "# if there are no scorable variants, end early\n",
    "SCORABLEVARIANTS=$(cat input.avinput.exonic_variant_function | wc -l || true)\n",
    "if [[ ${SCORABLEVARIANTS} -eq 0 ]]; then exit 0; fi\n",
    "if [[ {genome} == 'GRCh37' ]]; then\n",
    "    annovar/coding_change.pl input.avinput.exonic_variant_function annovar/humandb/hg19_wgEncodeGencodeBasicV33lift37.txt annovar/humandb/hg19_wgEncodeGencodeBasicV33lift37Mrna.fa --includesnp --onlyAltering --alltranscript > input.coding_changes.txt\n",
    "else\n",
    "    annovar/coding_change.pl input.avinput.exonic_variant_function annovar/humandb/hg38_wgEncodeGencodeBasicV33.txt annovar/humandb/hg38_wgEncodeGencodeBasicV33Mrna.fa --includesnp --onlyAltering --alltranscript > input.coding_changes.txt\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import official.nlp\n",
    "import official.nlp.keras_nlp.layers\n",
    "from transformers import TFT5EncoderModel, T5Tokenizer,T5Config\n",
    "import pandas\n",
    "pandas.options.mode.chained_assignment = None\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import scipy\n",
    "from scipy.stats import rankdata\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "approvedTranscripts=pandas.read_csv('gencodeBasicFullLengthTranscriptsConversionTable.txt',sep='\\t',low_memory=False)\n",
    "if genome=='GRCh38':\n",
    "    approvedTranscripts=pandas.read_csv('gencodeBasicFullLengthTranscriptsConversionTable_GRCh38.txt',sep='\\t',low_memory=False)\n",
    "\n",
    "canonical=pandas.read_csv('gnomad211_constraint_canonical_simple.txt',sep='\\t',low_memory=False)\n",
    "# remove the gnomad canonical transcripts that are not approvedTranscripts\n",
    "canonical=canonical.loc[canonical['transcript'].isin(approvedTranscripts['transcriptIDShort'].values),:].reset_index(drop=True)\n",
    "\n",
    "GTEx=pandas.read_csv('GTEx.V7.tx_medians.021820.tsv',sep='\\t',low_memory=False)\n",
    "# remove the non-approvedTranscripts from the expression data\n",
    "GTEx=GTEx.loc[GTEx['transcript_id'].isin(approvedTranscripts['transcriptIDShort'].values),:].reset_index(drop=True)\n",
    "# add a overall expression column\n",
    "GTEx['overallAvg']=GTEx.iloc[:,2:55].mean()\n",
    "\n",
    "sequences={}\n",
    "for record in SeqIO.parse(\"gencode.v33lift37.pc_translations.fa\",\"fasta\"):\n",
    "    transcriptID=record.id.split('|')[1]\n",
    "    if transcriptID in approvedTranscripts['transcriptID'].values:\n",
    "        sequences[transcriptID]=record.seq\n",
    "\n",
    "def groomAnnovarOutput(base,sequences=sequences,approvedTranscripts=approvedTranscripts,canonical=canonical,GTEx=GTEx,genome=genome):\n",
    "    sample=''\n",
    "    if genome=='GRCh37':\n",
    "        sample=pandas.read_csv(base + \".avinput.exonic_variant_function\",sep='\\t',low_memory=False,header=None,\n",
    "                            names=['line','varType','location','hg19_chr','hg19_pos(1-based)','end','ref','alt','genotype','qual','depth'])\n",
    "    else:\n",
    "        sample=pandas.read_csv(base + \".avinput.exonic_variant_function\",sep='\\t',low_memory=False,header=None,\n",
    "\t\t\t\t\t\t    names=['line','varType','location','hg38_chr','hg38_pos(1-based)','end','ref','alt','genotype','qual','depth'])\n",
    "    # convert the position, ref, and alt alleles to long form\n",
    "    longForm=pandas.read_csv(base + \"_locations.txt\",sep='\\t',low_memory=False,header=None,names=['chrom','pos_long','ref_long','alt_long'])\n",
    "    sample['lineNum']=sample.loc[:,'line'].str[4:].astype(int)-1\n",
    "    sample=sample.merge(longForm,how='inner',left_on='lineNum',right_on=longForm.index)\n",
    "    if genome=='GRCh37':\n",
    "        sample=sample.loc[:,['line','varType','location','hg19_chr','pos_long','end','ref_long','alt_long','genotype','qual','depth']].rename(columns={'pos_long':'hg19_pos(1-based)','ref_long':'ref','alt_long':'alt'}).reset_index(drop=True)\n",
    "    else:\n",
    "    \tsample=sample.loc[:,['line','varType','location','hg38_chr','pos_long','end','ref_long','alt_long','genotype','qual','depth']].rename(columns={'pos_long':'hg38_pos(1-based)','ref_long':'ref','alt_long':'alt'}).reset_index(drop=True)\n",
    "    # add new columns with placeholders to be filled in\n",
    "    sample['WildtypeSeq']=\"\"\n",
    "    sample['AltSeq']=\"\"\n",
    "    sample['ChangePos']=-1\n",
    "    sample['TranscriptID']=\"\"\n",
    "    sample['TranscriptIDShort']=sample['location'].str.split(':',expand=True)[1].str[:15]\n",
    "    sample['geneName']=sample['location'].str.split(':',expand=True)[0]\n",
    "    sample['geneID']=\"\"\n",
    "    sample['geneIDShort']=\"\"\n",
    "\n",
    "\n",
    "    for i in range(len(sample)):\n",
    "        if i % 1000 == 0:\n",
    "            print(str(i) + ' rows completed')\n",
    "        numTranscripts=len(sample.loc[i,'location'].split(','))\n",
    "        numCanonical=0\n",
    "        canonicals=[]\n",
    "        transcripts=[]\n",
    "        transcriptLengths=[]\n",
    "        canonicalTranscript=\"\"\n",
    "        correctedGeneName=\"\"\n",
    "        for j in range(numTranscripts-1):\n",
    "            if sample.loc[i,'location'].split(',')[j].split(':')[1][:15] in canonical['transcript'].values:\n",
    "                numCanonical=numCanonical+1\n",
    "                canonicals.append(sample.loc[i,'location'].split(',')[j].split(':')[1][:15])\n",
    "            if sample.loc[i,'location'].split(',')[j].split(':')[1] in approvedTranscripts['transcriptID'].values:  \n",
    "                transcripts.append(sample.loc[i,'location'].split(',')[j].split(':')[1][:15])\n",
    "                transcriptLengths.append(len(sequences[sample.loc[i,'location'].split(',')[j].split(':')[1]]))\n",
    "\n",
    "        if len(transcripts)>0:\n",
    "            if numCanonical==1:\n",
    "                transcriptID=canonicals[0]\n",
    "                sample.loc[i,'TranscriptIDShort']=transcriptID\n",
    "                sample.loc[i,'TranscriptID']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'transcriptID'].values[0]\n",
    "                sample.loc[i,'geneName']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneName'].values[0]\n",
    "                sample.loc[i,'geneID']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneID'].values[0]\n",
    "                sample.loc[i,'geneIDShort']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneIDShort'].values[0]\n",
    "            elif numCanonical==0:\n",
    "                if len(transcripts)==1:\n",
    "                    transcriptID=transcripts[0]\n",
    "                    sample.loc[i,'TranscriptIDShort']=transcriptID\n",
    "                    sample.loc[i,'TranscriptID']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'transcriptID'].values[0]\n",
    "                    sample.loc[i,'geneName']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneName'].values[0]\n",
    "                    sample.loc[i,'geneID']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneID'].values[0]\n",
    "                    sample.loc[i,'geneIDShort']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneIDShort'].values[0]\n",
    "                else:\n",
    "                    if len(GTEx.loc[GTEx['transcript_id'].isin(transcripts),:])>0:\n",
    "                        # pick the transcript with the highest expression\n",
    "                        transcriptID=GTEx.loc[GTEx['transcript_id'].isin(transcripts),:].sort_values(by=['overallAvg'],ascending=False).reset_index(drop=True).iloc[0,0]\n",
    "                        sample.loc[i,'TranscriptIDShort']=transcriptID\n",
    "                        sample.loc[i,'TranscriptID']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'transcriptID'].values[0]\n",
    "                        sample.loc[i,'geneName']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneName'].values[0]\n",
    "                        sample.loc[i,'geneID']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneID'].values[0]\n",
    "                        sample.loc[i,'geneIDShort']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneIDShort'].values[0]\n",
    "                    else:\n",
    "                        # if none of the transcripts have measured expression and none of them are canonical, then pick the one with the longest amino acid sequence\n",
    "                        # if multiple tie for longest, this picks the one we saw first\n",
    "                        j=transcriptLengths.index(max(transcriptLengths))\n",
    "                        transcriptID=transcripts[j]\n",
    "                        sample.loc[i,'TranscriptIDShort']=transcriptID\n",
    "                        sample.loc[i,'TranscriptID']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'transcriptID'].values[0]\n",
    "                        sample.loc[i,'geneName']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneName'].values[0]\n",
    "                        sample.loc[i,'geneID']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneID'].values[0]\n",
    "                        sample.loc[i,'geneIDShort']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneIDShort'].values[0]\n",
    "            elif numCanonical>1:\n",
    "                if len(GTEx.loc[GTEx['transcript_id'].isin(canonicals),:])>0:\n",
    "                    # pick the canonical transcript with the highest expression\n",
    "                    transcriptID=GTEx.loc[GTEx['transcript_id'].isin(canonicals),:].sort_values(by=['overallAvg'],ascending=False).reset_index(drop=True).iloc[0,0]\n",
    "                    sample.loc[i,'TranscriptIDShort']=transcriptID\n",
    "                    sample.loc[i,'TranscriptID']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'transcriptID'].values[0]\n",
    "                    sample.loc[i,'geneName']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneName'].values[0]\n",
    "                    sample.loc[i,'geneID']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneID'].values[0]\n",
    "                    sample.loc[i,'geneIDShort']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneIDShort'].values[0]\n",
    "                else:\n",
    "                    # if none of the canonical transcripts have measured expression, then pick the one with the longest amino acid sequence\n",
    "                    # if multiple tie for longest, this picks the one we saw first\n",
    "                    j=transcriptLengths.index(max(transcriptLengths))\n",
    "                    transcriptID=transcripts[j]\n",
    "                    sample.loc[i,'TranscriptIDShort']=transcriptID\n",
    "                    sample.loc[i,'TranscriptID']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'transcriptID'].values[0]\n",
    "                    sample.loc[i,'geneName']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneName'].values[0]\n",
    "                    sample.loc[i,'geneID']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneID'].values[0]\n",
    "                    sample.loc[i,'geneIDShort']=approvedTranscripts.loc[approvedTranscripts['transcriptIDShort']==transcriptID,'geneIDShort'].values[0]\n",
    "\n",
    "    for record in SeqIO.parse(base + \".coding_changes.txt\", \"fasta\"):\n",
    "        lineNum=record.id\n",
    "        # only use the transcript that we selected above \n",
    "        if sample.loc[sample['line']==lineNum,'TranscriptID'].values==record.description.split(' ')[1]:\n",
    "            if 'WILDTYPE' in record.description:\n",
    "                if record.seq.__str__()[:-1] == sequences[record.description.split(' ')[1]]:\n",
    "                    sample.loc[sample['line']==lineNum,'WildtypeSeq']=record.seq.__str__()\n",
    "                    sample.loc[sample['line']==lineNum,'TranscriptID']=record.description.split(' ')[1]\n",
    "            else:\n",
    "                sample.loc[sample['line']==lineNum,'AltSeq']=record.seq.__str__()\n",
    "                if 'startloss' in record.description:\n",
    "                    sample.loc[sample['line']==lineNum,'ChangePos']=1\n",
    "                elif 'silent' in record.description:\n",
    "                    sample.loc[sample['line']==lineNum,'ChangePos']=-1\n",
    "                else:\n",
    "                    sample.loc[sample['line']==lineNum,'ChangePos']=record.description.split(' ')[7].split('-')[0]\n",
    "    sample2=sample.loc[~((sample['WildtypeSeq']==\"\") | (sample['AltSeq']==\"\") | (sample['ChangePos']==-1)),:]\n",
    "    sample2.to_csv(base + '.groomed.txt',sep='\\t',index=False)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groomAnnovarOutput('input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "constraint=pandas.read_csv('gnomad211_constraint_canonical_simple.txt',sep='\\t',low_memory=False)\n",
    "\n",
    "gnomadAF=''\n",
    "CCR=''\n",
    "pext=''\n",
    "gerp=''\n",
    "if genome=='GRCh37':\n",
    "    gnomadAF=pandas.read_csv('gnomad211_exomes_AFs.txt',sep='\\t',low_memory=True,dtype={'hg19_chr':str,'hg19_pos(1-based)':np.int16,'ref':str,'alt':str,'AF':np.float16,'nhomalt':np.int16,'controls_AF':np.float16,'controls_nhomalt':np.int16})\n",
    "    gnomadAF.loc[gnomadAF['hg19_chr']=='X','hg19_chr']=23\n",
    "    gnomadAF.loc[gnomadAF['hg19_chr']=='Y','hg19_chr']=24\n",
    "    gnomadAF.loc[gnomadAF['hg19_chr']=='MT','hg19_chr']=25\n",
    "    gnomadAF['hg19_chr']=gnomadAF['hg19_chr'].astype(int)\n",
    "else:\n",
    "    gnomadAF=pandas.read_csv('gnomad211_GRCh38_exomes_AFs.txt',sep='\\t',low_memory=False,dtype={'hg38_chr':str,'hg38_pos(1-based)':np.int32,'ref':str,'alt':str,'AF':np.float32,'nhomalt':np.int32,'controls_AF':np.float32,'controls_nhomalt':np.int32})\n",
    "    gnomadAF=gnomadAF.loc[(~(gnomadAF['hg38_chr'].str.contains('_'))),:].reset_index(drop=True)\n",
    "    gnomadAF.loc[gnomadAF['hg38_chr']=='X','hg38_chr']=23\n",
    "    gnomadAF.loc[gnomadAF['hg38_chr']=='Y','hg38_chr']=24\n",
    "    gnomadAF.loc[gnomadAF['hg38_chr']=='MT','hg38_chr']=25\n",
    "    gnomadAF['hg38_chr']=gnomadAF['hg38_chr'].astype(int)\n",
    "\n",
    "\n",
    "if genome=='GRCh37':\n",
    "    CCR=pandas.read_csv('ccrs.enumerated.txt',sep='\\t',low_memory=True,dtype={'chrom':str,'pos':np.int16,'ccr_pct':np.float16})\n",
    "    CCR.loc[CCR['chrom']=='X','chrom']=23\n",
    "    CCR['chrom']=CCR.loc[:,'chrom'].astype(int)\n",
    "    CCR=CCR.sort_values(by=['chrom','pos','ccr_pct'],ascending=[True,True,False]).drop_duplicates(subset=['chrom','pos'],keep='first').reset_index(drop=True)\n",
    "else:\n",
    "    CCR=pandas.read_csv('ccrs_GRCh38.enumerated.txt',sep='\\t',low_memory=False,dtype={'chrom':str,'pos':np.int32,'ccr_pct':np.float32})\n",
    "    CCR=CCR.loc[(~(CCR['chrom'].str.contains('_'))),:]\n",
    "    CCR.loc[CCR['chrom']=='X','chrom']=23\n",
    "    CCR['chrom']=CCR.loc[:,'chrom'].astype(int)\n",
    "    CCR=CCR.sort_values(by=['chrom','pos','ccr_pct'],ascending=[True,True,False]).drop_duplicates(subset=['chrom','pos'],keep='first').reset_index(drop=True)\n",
    "\n",
    "if genome=='GRCh37':\n",
    "    pext=pandas.read_csv('gnomAD_pext_values.txt',sep='\\t',low_memory=True,dtype={'chr':str,'pos':np.int16,'pext':np.float16})\n",
    "    pext.loc[pext['chr']=='X','chr']=23\n",
    "    pext.loc[pext['chr']=='Y','chr']=24\n",
    "    pext.loc[pext['chr']=='MT','chr']=25\n",
    "    pext['chr']=pext.loc[:,'chr'].astype(int)\n",
    "    pext=pext.sort_values(by=['chr','pos','pext'],ascending=[True,True,False]).drop_duplicates(subset=['chr','pos'],keep='first').reset_index(drop=True)\n",
    "else:\n",
    "\tpext=pandas.read_csv('gnomAD_pext_values_GRCh38.txt',sep='\\t',low_memory=False,dtype={'chr':str,'pos':np.int32,'pext':np.float32})\n",
    "\tpext=pext.loc[(~(pext['chr'].str.contains('_'))),:]\n",
    "\tpext.loc[pext['chr']=='X','chr']=23\n",
    "\tpext.loc[pext['chr']=='Y','chr']=24\n",
    "\tpext.loc[pext['chr']=='MT','chr']=25\n",
    "\tpext['chr']=pext.loc[:,'chr'].astype(int)\n",
    "\tpext=pext.sort_values(by=['chr','pos','pext'],ascending=[True,True,False]).drop_duplicates(subset=['chr','pos'],keep='first').reset_index(drop=True)\n",
    "\n",
    "if genome=='GRCh37':\n",
    "    gerp=pandas.read_csv('gerpOnExons.txt',sep='\\t',low_memory=True,header=None,names=['chr','pos','gerp'],dtype={'chr':str,'pos':np.int16,'gerp':np.float16})\n",
    "    gerp.loc[gerp['chr']=='X','chr']=23\n",
    "    gerp.loc[gerp['chr']=='Y','chr']=24\n",
    "    gerp.loc[gerp['chr']=='MT','chr']=25\n",
    "    gerp['chr']=gerp['chr'].astype(int)\n",
    "    gerp=gerp.sort_values(by=['chr','pos','gerp'],ascending=[True,True,False]).drop_duplicates(subset=['chr','pos'],keep='first').reset_index(drop=True)\n",
    "else:\n",
    "\tgerp=pandas.read_csv('gerpOnExons_GRCh38.txt',sep='\\t',low_memory=False,header=None,names=['chr','pos','gerp'],dtype={'chr':str,'pos':np.int32,'gerp':np.float32})\n",
    "\tgerp=gerp.loc[(~(gerp['chr'].str.contains('_'))),:]\n",
    "\tgerp.loc[gerp['chr']=='X','chr']=23\n",
    "\tgerp.loc[gerp['chr']=='Y','chr']=24\n",
    "\tgerp.loc[gerp['chr']=='MT','chr']=25\n",
    "\tgerp['chr']=gerp['chr'].astype(int)\n",
    "\tgerp=gerp.sort_values(by=['chr','pos','gerp'],ascending=[True,True,False]).drop_duplicates(subset=['chr','pos'],keep='first').reset_index(drop=True)\n",
    "\n",
    "GDI=pandas.read_csv('GDI.groomed.txt',sep='\\t',low_memory=False)\n",
    "RVIS=pandas.read_csv('RVIS.groomed.txt',sep='\\t',low_memory=False)\n",
    "\n",
    "def annotateVariants(base,constraint=constraint,gnomadAF=gnomadAF,CCR=CCR,pext=pext,gerp=gerp,GDI=GDI,RVIS=RVIS,genome=genome):\n",
    "    import pandas\n",
    "    import numpy as np\n",
    "    sample=pandas.read_csv(base + '.groomed.txt',sep='\\t',low_memory=False)\n",
    "    if genome=='GRCh37':\n",
    "        sample.loc[sample['hg19_chr']=='X','hg19_chr']=23\n",
    "        sample.loc[sample['hg19_chr']=='Y','hg19_chr']=24\n",
    "        sample.loc[sample['hg19_chr']=='MT','hg19_chr']=25\n",
    "        sample['hg19_chr']=sample['hg19_chr'].astype(int)\n",
    "    else:\n",
    "        sample.loc[sample['hg38_chr']=='X','hg38_chr']=23\n",
    "        sample.loc[sample['hg38_chr']=='Y','hg38_chr']=24\n",
    "        sample.loc[sample['hg38_chr']=='MT','hg38_chr']=25\n",
    "        sample.loc[(~(sample['hg38_chr'].str.contains('_'))),:].reset_index(drop=True)\n",
    "        sample['hg38_chr']=sample['hg38_chr'].astype(int)\n",
    "\n",
    "    # merge on the allele frequency data\n",
    "    if genome=='GRCh37':\n",
    "        sample=sample.merge(gnomadAF,how='left',on=['hg19_chr','hg19_pos(1-based)','ref','alt'])\n",
    "    else:\n",
    "        sample=sample.merge(gnomadAF,how='left',on=['hg38_chr','hg38_pos(1-based)','ref','alt'])\n",
    "\n",
    "    # merge on the constraint data (try transcript ID merge first)\n",
    "    sampleTranscript=sample.merge(constraint,how='inner',left_on=['TranscriptIDShort'],right_on=['transcript'])\n",
    "    notMatched=sample.loc[~(sample['TranscriptIDShort'].isin(sampleTranscript['TranscriptIDShort'])),:]\n",
    "    constraint=pandas.read_csv('gnomad211_constraint_simple_geneLevel.txt',sep='\\t',low_memory=False)\n",
    "    sampleGeneID=notMatched.merge(constraint,how='inner',left_on=['geneIDShort'],right_on=['gene_id'])\n",
    "    notMatched2=notMatched.loc[~(notMatched['geneIDShort'].isin(sampleGeneID['geneIDShort'])),:]\n",
    "    sampleGeneName=notMatched2.merge(constraint,how='left',left_on=['geneName'],right_on=['gene'])\n",
    "    # stack them all back together\n",
    "    sample2=pandas.concat([sampleTranscript,sampleGeneID,sampleGeneName],axis=0,ignore_index=True)\n",
    "    if genome=='GRCh37':\n",
    "        sample2.loc[sample2['hg19_chr']=='X','hg19_chr']=23\n",
    "        sample2.loc[sample2['hg19_chr']=='Y','hg19_chr']=24\n",
    "        sample2.loc[sample2['hg19_chr']=='MT','hg19_chr']=25\n",
    "        sample2['hg19_chr']=sample2['hg19_chr'].astype(int)\n",
    "    else:\n",
    "        sample2.loc[sample2['hg38_chr']=='X','hg38_chr']=23\n",
    "        sample2.loc[sample2['hg38_chr']=='Y','hg38_chr']=24\n",
    "        sample2.loc[sample2['hg38_chr']=='MT','hg38_chr']=25\n",
    "        sample2['hg38_chr']=sample2['hg38_chr'].astype(int)\n",
    "\n",
    "    # merge on the CCR data\n",
    "    sample2['CCR']=np.nan\n",
    "    sampleSNVs=sample2.loc[sample2['varType'].isin(['nonsynonymous SNV','synonymous SNV','stopgain','stoploss']),['hg19_chr','hg19_pos(1-based)']]\n",
    "    sampleIndels=sample2.loc[sample2['varType'].isin(['frameshift insertion','frameshift deletion','frameshift substitution',\n",
    "                                                    'nonframeshift insertion','nonframeshift deletion','nonframeshift substitution']),['hg19_chr','hg19_pos(1-based)','ref']]\n",
    "    sampleIndels['length']=sampleIndels['ref'].str.len()\n",
    "    sampleIndels['CCR']=np.nan\n",
    "    if genome=='GRCh37':\n",
    "        sampleSNVs2=sampleSNVs.merge(CCR,how='left',left_on=['hg19_chr','hg19_pos(1-based)'],right_on=['chrom','pos']).set_index(sampleSNVs.index)\n",
    "    else:\n",
    "        sampleSNVs2=sampleSNVs.merge(CCR,how='left',left_on=['hg38_chr','hg38_pos(1-based)'],right_on=['chrom','pos']).set_index(sampleSNVs.index)\n",
    "    for i in range(len(sampleIndels)):\n",
    "        if i%100==0:\n",
    "            print(str(i) + ' rows complete of ' + str(len(sampleIndels)))\n",
    "        startPos=sampleIndels.iloc[i,1]+1\n",
    "        endPos=startPos+sampleIndels.iloc[i,3]\n",
    "        sampleIndels.iloc[i,4]=CCR.loc[((CCR['chrom']==sampleIndels.iloc[i,0]) & (CCR['pos'].isin(range(startPos,endPos)))),'ccr_pct'].max()\n",
    "    sample2.loc[sampleSNVs2.index,'CCR']=sampleSNVs2.loc[:,'ccr_pct'].values\n",
    "    sample2.loc[sampleIndels.index,'CCR']=sampleIndels.loc[:,'CCR'].values\n",
    "\n",
    "    # merge on the pext data\n",
    "    sample2['pext']=np.nan\n",
    "    sampleIndels['pext']=np.nan\n",
    "    if genome=='GRCh37':\n",
    "        sampleSNVs2=sampleSNVs.merge(pext,how='left',left_on=['hg19_chr','hg19_pos(1-based)'],right_on=['chr','pos']).set_index(sampleSNVs.index)\n",
    "    else:\n",
    "        sampleSNVs2=sampleSNVs.merge(pext,how='left',left_on=['hg38_chr','hg38_pos(1-based)'],right_on=['chr','pos']).set_index(sampleSNVs.index)\n",
    "    for i in range(len(sampleIndels)):\n",
    "        if i%100==0:\n",
    "            print(str(i) + ' rows complete of ' + str(len(sampleIndels)))\n",
    "        startPos=sampleIndels.iloc[i,1]+1\n",
    "        endPos=startPos+sampleIndels.iloc[i,3]\n",
    "        sampleIndels.iloc[i,5]=pext.loc[((pext['chr']==sampleIndels.iloc[i,0]) & (pext['pos'].isin(range(startPos,endPos)))),'pext'].max()\n",
    "    sample2.loc[sampleSNVs2.index,'pext']=sampleSNVs2.loc[:,'pext'].values\n",
    "    sample2.loc[sampleIndels.index,'pext']=sampleIndels.loc[:,'pext'].values\n",
    "\n",
    "    # merge on the GERP data\n",
    "    sample2['gerp']=np.nan\n",
    "    sampleIndels['gerp']=np.nan\n",
    "    if genome=='GRCh37':\n",
    "        sampleSNVs2=sampleSNVs.merge(gerp,how='left',left_on=['hg19_chr','hg19_pos(1-based)'],right_on=['chr','pos']).set_index(sampleSNVs.index)\n",
    "    else:\n",
    "        sampleSNVs2=sampleSNVs.merge(gerp,how='left',left_on=['hg38_chr','hg38_pos(1-based)'],right_on=['chr','pos']).set_index(sampleSNVs.index)\n",
    "    for i in range(len(sampleIndels)):\n",
    "        if i%100==0:\n",
    "            print(str(i) + ' rows complete of ' + str(len(sampleIndels)))\n",
    "        startPos=sampleIndels.iloc[i,1]+1\n",
    "        endPos=startPos+sampleIndels.iloc[i,3]\n",
    "        sampleIndels.iloc[i,6]=gerp.loc[((gerp['chr']==sampleIndels.iloc[i,0]) & (gerp['pos'].isin(range(startPos,endPos)))),'gerp'].max()\n",
    "    sample2.loc[sampleSNVs2.index,'gerp']=sampleSNVs2.loc[:,'gerp'].values\n",
    "    sample2.loc[sampleIndels.index,'gerp']=sampleIndels.loc[:,'gerp'].values\n",
    "\n",
    "    if genome=='GRCh37':\n",
    "        sample2=sample2.drop_duplicates(subset=['hg19_chr','hg19_pos(1-based)','ref','alt'],keep='first')\n",
    "        sample2=sample2.drop(columns=['line','location','end','qual','depth','gene','transcript', 'canonical','gene_id'])\n",
    "        sample2=sample2.sort_values(by=['hg19_chr','hg19_pos(1-based)','ref','alt']).reset_index(drop=True)\n",
    "    else:\n",
    "        sample2=sample2.drop_duplicates(subset=['hg38_chr','hg38_pos(1-based)','ref','alt'],keep='first')\n",
    "        sample2=sample2.drop(columns=['line','location','end','qual','depth','gene','transcript', 'canonical','gene_id'])\n",
    "        sample2=sample2.sort_values(by=['hg38_chr','hg38_pos(1-based)','ref','alt']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # merge on GDI data\n",
    "    sample2=sample2.merge(GDI,how='left',on='geneName')\n",
    "    # merge on RVIS data\n",
    "    sample2=sample2.merge(RVIS,how='left',on='geneName')\n",
    "    \n",
    "    if genome=='GRCh37':\n",
    "        sample2=sample2.sort_values(by=['hg19_chr','hg19_pos(1-based)','ref','alt']).reset_index(drop=True)\n",
    "        sample2=sample2.drop_duplicates(subset=['hg19_chr','hg19_pos(1-based)','ref','alt'],keep='first').reset_index(drop=True)\n",
    "    else:\n",
    "        sample2=sample2.sort_values(by=['hg38_chr','hg38_pos(1-based)','ref','alt']).reset_index(drop=True)\n",
    "        sample2=sample2.drop_duplicates(subset=['hg38_chr','hg38_pos(1-based)','ref','alt'],keep='first').reset_index(drop=True)\n",
    "\n",
    "    sample2.to_csv(base + '.annotated.txt',sep='\\t',index=False)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotateVariants('input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, list_IDs, labels, dataFrameIn, tokenizer, T5Model, batch_size=32, padding=100, n_channels_emb=1024, n_channels_mm=51, n_classes=3, shuffle=True):\n",
    "        self.padding = padding\n",
    "        self.dim = self.padding + self.padding + 1\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels_emb = n_channels_emb\n",
    "        self.n_channels_mm = n_channels_mm\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.dataFrameIn=dataFrameIn\n",
    "        self.tokenizer = tokenizer\n",
    "        self.T5Model = T5Model\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        if (len(self.list_IDs) % self.batch_size) == 0:\n",
    "            return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "        else:\n",
    "            return int(np.ceil(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        if (((len(self.list_IDs) % self.batch_size) != 0) & (((index+1)*self.batch_size)>len(self.list_IDs))):\n",
    "            indexes = self.indexes[index*self.batch_size:]\n",
    "        else:\n",
    "            indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        thisBatchSize=len(list_IDs_temp)\n",
    "        altEmbeddings=np.zeros((thisBatchSize, self.dim, self.n_channels_emb))\n",
    "        mm_alt=np.zeros((thisBatchSize, self.dim, self.n_channels_mm))\n",
    "        mm_orig=np.zeros((thisBatchSize, self.dim, self.n_channels_mm))\n",
    "        nonSeq=np.zeros((thisBatchSize, 12))\n",
    "        y = np.empty((thisBatchSize), dtype=int)\n",
    "        AMINO_ACIDS = {'A':0,'C':1,'D':2,'E':3,'F':4,'G':5,'H':6,'I':7,'K':8,'L':9,'M':10,'N':11,'P':12,'Q':13,'R':14,'S':15,'T':16,'V':17,'W':18,'Y':19} \n",
    "        T5AltSeqTokens=[]\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # process Alt seq with T5 model to create embeddings\n",
    "            transcriptID=self.dataFrameIn.loc[ID,'TranscriptID']\n",
    "            changePos=self.dataFrameIn.loc[ID,'ChangePos']-1\n",
    "            if changePos<0:\n",
    "                changePos=0\n",
    "            AltSeq=self.dataFrameIn.loc[ID,'AltSeq']\n",
    "            if AltSeq[-1]!=\"*\":\n",
    "                AltSeq=AltSeq + \"*\"\n",
    "            seqLenAlt=len(AltSeq)-1\n",
    "            startPos=0\n",
    "            if changePos>self.padding:\n",
    "                if (changePos+self.padding)<seqLenAlt:\n",
    "                    startPos=changePos-self.padding\n",
    "                elif seqLenAlt>=self.dim:\n",
    "                    startPos=seqLenAlt-self.dim\n",
    "            endPos=changePos+self.padding\n",
    "            if changePos<self.padding:\n",
    "                if self.dim<seqLenAlt:\n",
    "                    endPos=self.dim\n",
    "                else:\n",
    "                    endPos=seqLenAlt\n",
    "            elif (changePos+self.padding)>=seqLenAlt:\n",
    "                endPos=seqLenAlt\n",
    "            T5AltSeqTokens.append(\" \".join(AltSeq[startPos:endPos]))\n",
    "            # prep the WT seq too\n",
    "            WTSeq=self.dataFrameIn.loc[ID,'WildtypeSeq']\n",
    "            if WTSeq[-1]!=\"*\":\n",
    "                WTSeq=WTSeq + \"*\"\n",
    "            seqLen=len(WTSeq)-1\n",
    "            startPos=0\n",
    "            if changePos>self.padding:\n",
    "                if (changePos+self.padding)<seqLen:\n",
    "                    startPos=int(changePos-self.padding)\n",
    "                elif seqLen>=self.dim:\n",
    "                    startPos=int(seqLen-self.dim)\n",
    "            endPos=int(changePos+self.padding)\n",
    "            if changePos<self.padding:\n",
    "                if self.dim<seqLen:\n",
    "                    endPos=int(self.dim)\n",
    "                else:\n",
    "                    endPos=int(seqLen)\n",
    "            elif (changePos+self.padding)>=seqLen:\n",
    "                endPos=int(seqLen)\n",
    "            T5AltSeqTokens.append(\" \".join(WTSeq[startPos:endPos]))\n",
    "\n",
    "\n",
    "            # collect MMSeqs WT info\n",
    "            tmp=np.load(\"HHMFiles/\" + transcriptID + \"_MMSeqsProfile.npz\",allow_pickle=True)\n",
    "            tmp=tmp['arr_0']\n",
    "            seqLen=tmp.shape[0]\n",
    "            startPos=changePos-self.padding\n",
    "            endPos=changePos+self.padding + 1\n",
    "            startOffset=0\n",
    "            endOffset=self.dim\n",
    "            if changePos<self.padding:\n",
    "                startPos=0\n",
    "                startOffset=self.padding-changePos\n",
    "            if (changePos + self.padding) >= seqLen:\n",
    "                endPos=seqLen\n",
    "                endOffset=self.padding + seqLen - changePos\n",
    "            mm_orig[i,startOffset:endOffset,:] = tmp[startPos:endPos,:]\n",
    "\n",
    "            # collect MMSeqs Alt info\n",
    "            # change the amino acid at 'ChangePos' and any after that if needed\n",
    "            varType=self.dataFrameIn.loc[ID,'varType']\n",
    "            WTSeq=self.dataFrameIn.loc[ID,'WildtypeSeq']\n",
    "            if varType=='nonsynonymous SNV':\n",
    "                if changePos==0:\n",
    "                    # then this transcript is ablated\n",
    "                    altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    altEncoded[:,0:20]=0\n",
    "                    altEncoded[:,50]=0\n",
    "                else:\n",
    "                    # change the single amino acid\n",
    "                    altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    altEncoded[changePos,AMINO_ACIDS[WTSeq[changePos]]]=0\n",
    "                    altEncoded[changePos,AMINO_ACIDS[AltSeq[changePos]]]=1\n",
    "            elif varType=='stopgain':\n",
    "                if changePos==0:\n",
    "                    # then this transcript is ablated\n",
    "                    altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    altEncoded[:,0:20]=0\n",
    "                    altEncoded[:,50]=0\n",
    "                elif seqLenAlt>seqLen:\n",
    "                    altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    for j in range(changePos,seqLen):\n",
    "                        altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    for j in range(seqLen,seqLenAlt):\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    altEncoded[seqLen:,50]=1\n",
    "                else:\n",
    "                    altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    altEncoded[changePos:,0:20]=0\n",
    "                    altEncoded[changePos:,50]=0\n",
    "            elif varType=='stoploss':\n",
    "                altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                altEncoded[:seqLen,:]=tmp\n",
    "                for j in range(seqLen,seqLenAlt):\n",
    "                    altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                altEncoded[seqLen:,50]=1\n",
    "            elif varType=='synonymous SNV':\n",
    "                # no change\n",
    "                altEncoded=tmp\n",
    "            elif ((varType=='frameshift deletion') | (varType=='frameshift insertion') | (varType=='frameshift substitution')):\n",
    "                if seqLen<seqLenAlt:\n",
    "                    altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    for j in range(changePos,seqLen):\n",
    "                        altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    for j in range(seqLen,seqLenAlt):\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    altEncoded[seqLen:,50]=1\n",
    "                elif seqLen>seqLenAlt:\n",
    "                    for j in range(changePos,seqLenAlt):\n",
    "                        tmp[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        tmp[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    for j in range(seqLenAlt,seqLen):\n",
    "                        tmp[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                    altEncoded=tmp\n",
    "                elif seqLen==seqLenAlt:\n",
    "                    for j in range(changePos,seqLen):\n",
    "                        tmp[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        tmp[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    altEncoded=tmp\n",
    "                else:\n",
    "                    print('Error: seqLen comparisons did not work')\n",
    "                    exit()\n",
    "            elif varType=='nonframeshift deletion':\n",
    "                # how many amino acids deleted?\n",
    "                altNucLen=0\n",
    "                if self.dataFrameIn.loc[ID,'alt']!='-':\n",
    "                    altNucLen=len(self.dataFrameIn.loc[ID,'alt'])\n",
    "                refNucLen=len(self.dataFrameIn.loc[ID,'ref'])\n",
    "                numAADel=int((refNucLen-altNucLen)/3)\n",
    "                if (seqLen-numAADel)==seqLenAlt:\n",
    "                    # non-frameshift deletion\n",
    "                    #altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                    #altEncoded[:changePos,:]=tmp[:changePos,:]\n",
    "                    #altEncoded[changePos:,:]=tmp[(changePos+numAADel):,:]\n",
    "                    for j in range(changePos,(changePos+numAADel)):\n",
    "                        tmp[j,:20]=0\n",
    "                    altEncoded=tmp\n",
    "                elif seqLen>=seqLenAlt:\n",
    "                    # early truncation\n",
    "                    altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    for j in range(changePos,seqLenAlt):\n",
    "                        altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    #for j in range(seqLenAlt,seqLen):\n",
    "                    #    altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                    altEncoded[seqLenAlt:,0:20]=0\n",
    "                    altEncoded[seqLenAlt:,50]=0\n",
    "                elif seqLen<seqLenAlt:\n",
    "                    # deletion causes stop-loss\n",
    "                    altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    for j in range(changePos,seqLen):\n",
    "                        altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    #for j in range(seqLen,seqLenAlt):\n",
    "                    #    altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    altEncoded[seqLen:,0:20]=0\n",
    "                    altEncoded[seqLen:,50]=0\n",
    "                else:\n",
    "                    print('Error: seqLen comparisons did not work for nonframeshift deletion')\n",
    "                    exit()\n",
    "            elif varType=='nonframeshift insertion':\n",
    "                # how many amino acids inserted?\n",
    "                refNucLen=0\n",
    "                if self.dataFrameIn.loc[ID,'ref']!='-':\n",
    "                    altNucLen=len(self.dataFrameIn.loc[ID,'ref'])\n",
    "                altNucLen=len(self.dataFrameIn.loc[ID,'alt'])\n",
    "                numAAIns=int((altNucLen-refNucLen)/3)\n",
    "                if (seqLen+numAAIns)==seqLenAlt:\n",
    "                    # non-frameshift insertion\n",
    "                    altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                    altEncoded[:changePos,:]=tmp[:changePos,:]\n",
    "                    altEncoded[(changePos+numAAIns):,:]=tmp[changePos:,:]\n",
    "                    for j in range(numAAIns):\n",
    "                        altEncoded[(changePos+j),AMINO_ACIDS[AltSeq[(changePos+j)]]]=1\n",
    "                    altEncoded[:,50]=1\n",
    "                elif seqLen<seqLenAlt:\n",
    "                    # stop loss\n",
    "                    altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    for j in range(changePos,seqLen):\n",
    "                        altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    for j in range(seqLen,seqLenAlt):\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    altEncoded[seqLen:,50]=1\n",
    "                elif seqLen>=seqLenAlt:\n",
    "                    # stop gain\n",
    "                    altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    for j in range(changePos,seqLenAlt):\n",
    "                        altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    altEncoded[seqLenAlt:,0:20]=0\n",
    "                    altEncoded[seqLenAlt:,50]=0\n",
    "                else:\n",
    "                    print('Error: seqLen comparisons did not work for nonframeshift insertion')\n",
    "                    exit()\n",
    "            elif varType=='nonframeshift substitution':\n",
    "                # is this an insertion or a deletion?\n",
    "                # note that there will not be any '-' symbols in these ref or alt fields because it is a substitution\n",
    "                refNucLen=len(self.dataFrameIn.loc[ID,'ref'])\n",
    "                altNucLen=len(self.dataFrameIn.loc[ID,'alt'])\n",
    "                if refNucLen>altNucLen:\n",
    "                    # deletion\n",
    "                    # does this cause an early truncation or non-frameshift deletion?\n",
    "                    if seqLen>seqLenAlt: \n",
    "                        numAADel=int((refNucLen-altNucLen)/3)\n",
    "                        if (seqLen-numAADel)==seqLenAlt:\n",
    "                            # non-frameshift deletion\n",
    "                            #altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                            #altEncoded[:changePos,:]=tmp[:changePos,:]\n",
    "                            #altEncoded[changePos:,:]=tmp[(changePos+numAADel):,:]\n",
    "                            for j in range(changePos,(changePos+numAADel)):\n",
    "                                tmp[j,:20]=0\n",
    "                            altEncoded=tmp\n",
    "                        else:\n",
    "                            # early truncation\n",
    "                            altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                            altEncoded[:seqLen,:]=tmp\n",
    "                            for j in range(changePos,seqLenAlt):\n",
    "                                altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                                altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                            #for j in range(seqLenAlt,seqLen):\n",
    "                            #    altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                            altEncoded[seqLenAlt:,0:20]=0\n",
    "                            altEncoded[seqLenAlt:,50]=0\n",
    "                    # does this cause a stop loss?\n",
    "                    elif seqLen<seqLenAlt:\n",
    "                        altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                        altEncoded[:seqLen,:]=tmp\n",
    "                        for j in range(changePos,seqLen):\n",
    "                            altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                        for j in range(seqLen,seqLenAlt):\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                        altEncoded[seqLen:,50]=1\n",
    "                    else: # not sure how this would happen\n",
    "                        altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                        altEncoded[:seqLen,:]=tmp\n",
    "                        for j in range(changePos,seqLen):\n",
    "                            altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                elif refNucLen<altNucLen:\n",
    "                    # insertion\n",
    "                    # does this cause a stop loss or non-frameshift insertion?\n",
    "                    if seqLen<seqLenAlt: \n",
    "                        numAAIns=int((altNucLen-refNucLen)/3)\n",
    "                        if (seqLen+numAAIns)==seqLenAlt:\n",
    "                            # non-frameshift insertion\n",
    "                            altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                            altEncoded[:changePos,:]=tmp[:changePos,:]\n",
    "                            altEncoded[(changePos+numAAIns):,:]=tmp[changePos:,:]\n",
    "                            for j in range(numAAIns):\n",
    "                                altEncoded[(changePos+j),AMINO_ACIDS[AltSeq[(changePos+j)]]]=1\n",
    "                            altEncoded[:,50]=1\n",
    "                        else:\n",
    "                            # stop loss\n",
    "                            altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                            altEncoded[:seqLen,:]=tmp\n",
    "                            for j in range(changePos,seqLen):\n",
    "                                altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                                altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                            for j in range(seqLen,seqLenAlt):\n",
    "                                altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                            altEncoded[:,50]=1\n",
    "                    # does this cause an early truncation?\n",
    "                    elif seqLen>seqLenAlt: \n",
    "                        altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                        altEncoded[:seqLen,:]=tmp\n",
    "                        for j in range(changePos,seqLenAlt):\n",
    "                            altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                        altEncoded[seqLenAlt:,0:20]=0\n",
    "                        #for j in range(seqLenAlt,seqLen):\n",
    "                        #    altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[seqLenAlt:,50]=0\n",
    "                    else: # not sure how this would happen\n",
    "                        altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                        altEncoded[:seqLen,:]=tmp\n",
    "                        for j in range(changePos,seqLen):\n",
    "                            altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                elif refNucLen==altNucLen:\n",
    "                    if seqLen==seqLenAlt:\n",
    "                        # synonymous or nonsynonymous change\n",
    "                        altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                        altEncoded[:seqLen,:]=tmp\n",
    "                        altEncoded[changePos,AMINO_ACIDS[WTSeq[changePos]]]=0\n",
    "                        altEncoded[changePos,AMINO_ACIDS[AltSeq[changePos]]]=1\n",
    "                    elif seqLen>seqLenAlt:\n",
    "                        # early truncation\n",
    "                        altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                        altEncoded[:seqLen,:]=tmp\n",
    "                        for j in range(changePos,seqLenAlt):\n",
    "                            altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                        altEncoded[seqLenAlt:,0:20]=0\n",
    "                        #for j in range(seqLenAlt,seqLen):\n",
    "                        #    altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[seqLenAlt:,50]=0\n",
    "                    elif seqLen<seqLenAlt:\n",
    "                        # stop loss\n",
    "                        altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                        altEncoded[:seqLen,:]=tmp\n",
    "                        for j in range(changePos,seqLen):\n",
    "                            altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                        for j in range(seqLen,seqLenAlt):\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                        altEncoded[seqLen:,50]=1\n",
    "                    else:\n",
    "                        print('non-frameshift substitution comparisons failed')\n",
    "                        exit()\n",
    "                else:\n",
    "                    print('Error: nonframeshift substitution nucleotide length comparison did not work')\n",
    "                    exit()\n",
    "            startPos=changePos-self.padding\n",
    "            endPos=changePos+self.padding+1\n",
    "            startOffset=0\n",
    "            endOffset=self.dim\n",
    "            if changePos<self.padding:\n",
    "                startPos=0\n",
    "                startOffset=self.padding-changePos\n",
    "            if (changePos + self.padding) >= seqLenAlt:\n",
    "                endPos=seqLenAlt\n",
    "                endOffset=self.padding + seqLenAlt - changePos\n",
    "            # exception to deal with start loss SNVs that create new frameshifted products longer than the original protein (when original was shorter than padding length)\n",
    "            if ((changePos==0) & (self.padding>=seqLen) & (seqLen<seqLenAlt) & (varType=='nonsynonymous SNV')):\n",
    "                endPos=seqLen\n",
    "                endOffset=self.padding + seqLen - changePos\n",
    "            elif ((changePos==0) & (varType=='stopgain')): # related exception for stopgains at position 0\n",
    "                if (seqLen+self.padding)<=self.dim:\n",
    "                    endPos=seqLen\n",
    "                    endOffset=self.padding + seqLen - changePos\n",
    "                else:\n",
    "                    endPos=self.padding+1\n",
    "                    endOffset=self.dim\n",
    "            mm_alt[i,startOffset:endOffset,:] = altEncoded[startPos:endPos,:]\n",
    "\n",
    "\n",
    "            # non-seq info\n",
    "            nonSeq[i] = self.dataFrameIn.loc[ID,['controls_AF','controls_nhomalt','pLI','pNull','pRec','mis_z','lof_z','CCR','GDI','pext','RVIS_ExAC_0.05','gerp']]\n",
    "            \n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        # process the altSeq and wtSeq through the T5 tokenizer (for consistency with pre-computed data used for training)\n",
    "        allTokens=self.tokenizer.batch_encode_plus(T5AltSeqTokens,add_special_tokens=True, padding=True, return_tensors=\"tf\")\n",
    "        input_ids=allTokens['input_ids'][::2]\n",
    "        attnMask=allTokens['attention_mask'][::2]\n",
    "        # but only process the altSeq through the T5 model\n",
    "        #embeddings=self.T5Model(input_ids[::2],decoder_input_ids=input_ids[::2])\n",
    "        embeddings=self.T5Model(input_ids,attention_mask=attnMask)\n",
    "        allEmbeddings=np.asarray(embeddings.last_hidden_state)\n",
    "        for i in range(thisBatchSize):\n",
    "            seq_len = (np.asarray(attnMask)[i] == 1).sum()\n",
    "            seq_emb = allEmbeddings[i][1:seq_len-1]\n",
    "            altEmbeddings[i,:seq_emb.shape[0],:]=seq_emb\n",
    "\n",
    "\n",
    "        X={'alt_cons':mm_alt,'alt_emb':altEmbeddings,'non_seq_info':nonSeq,'mm_orig_seq':mm_orig}\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaverickArchitecture1(input_shape=201,classes=3,classifier_activation='softmax',**kwargs):\n",
    "    input0 = tf.keras.layers.Input(shape=(input_shape,51),name='mm_orig_seq')\n",
    "    input1 = tf.keras.layers.Input(shape=(input_shape,51),name='mm_alt_seq')\n",
    "    input2 = tf.keras.layers.Input(shape=12,name='non_seq_info')\n",
    "\n",
    "    # project input to an embedding size that is easier to work with\n",
    "    x_orig = tf.keras.layers.experimental.EinsumDense('...x,xy->...y',output_shape=64,bias_axes='y')(input0)\n",
    "    x_alt = tf.keras.layers.experimental.EinsumDense('...x,xy->...y',output_shape=64,bias_axes='y')(input1)\n",
    "\n",
    "    posEnc_wt = official.nlp.keras_nlp.layers.PositionEmbedding(max_length=input_shape)(x_orig)\n",
    "    x_orig = tf.keras.layers.Masking()(x_orig)\n",
    "    x_orig = tf.keras.layers.Add()([x_orig,posEnc_wt])\n",
    "    x_orig = tf.keras.layers.LayerNormalization(axis=-1, epsilon=1e-12,dtype=tf.float32)(x_orig)\n",
    "    x_orig = tf.keras.layers.Dropout(0.05)(x_orig)\n",
    "\n",
    "    posEnc_alt = official.nlp.keras_nlp.layers.PositionEmbedding(max_length=input_shape)(x_alt)\n",
    "    x_alt = tf.keras.layers.Masking()(x_alt)\n",
    "    x_alt = tf.keras.layers.Add()([x_alt,posEnc_alt])\n",
    "    x_alt = tf.keras.layers.LayerNormalization(axis=-1, epsilon=1e-12,dtype=tf.float32)(x_alt)\n",
    "    x_alt = tf.keras.layers.Dropout(0.05)(x_alt)\n",
    "\n",
    "    transformer1 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer2 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer3 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer4 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer5 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer6 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    \n",
    "    x_orig = transformer1(x_orig)\n",
    "    x_orig = transformer2(x_orig)\n",
    "    x_orig = transformer3(x_orig)\n",
    "    x_orig = transformer4(x_orig)\n",
    "    x_orig = transformer5(x_orig)\n",
    "    x_orig = transformer6(x_orig)\n",
    "    \n",
    "    x_alt = transformer1(x_alt)\n",
    "    x_alt = transformer2(x_alt)\n",
    "    x_alt = transformer3(x_alt)\n",
    "    x_alt = transformer4(x_alt)\n",
    "    x_alt = transformer5(x_alt)\n",
    "    x_alt = transformer6(x_alt)\n",
    "\n",
    "    first_token_tensor_orig = (tf.keras.layers.Lambda(lambda a: tf.squeeze(a[:, 100:101, :], axis=1))(x_orig))\n",
    "    x_orig = tf.keras.layers.Dense(units=64,activation='tanh')(first_token_tensor_orig)\n",
    "    x_orig = tf.keras.layers.Dropout(0.05)(x_orig)\n",
    "\n",
    "    first_token_tensor_alt = (tf.keras.layers.Lambda(lambda a: tf.squeeze(a[:, 100:101, :], axis=1))(x_alt))\n",
    "    x_alt = tf.keras.layers.Dense(units=64,activation='tanh')(first_token_tensor_alt)\n",
    "    x_alt = tf.keras.layers.Dropout(0.05)(x_alt)\n",
    "\n",
    "    diff = tf.keras.layers.Subtract()([x_alt,x_orig])\n",
    "    combined = tf.keras.layers.concatenate([x_alt,diff])\n",
    "\n",
    "    input2Dense1 = tf.keras.layers.Dense(64,activation='relu')(input2)\n",
    "    input2Dense1 = tf.keras.layers.Dropout(0.05)(input2Dense1)\n",
    "    x = tf.keras.layers.concatenate([combined,input2Dense1])\n",
    "    x = tf.keras.layers.Dropout(0.05)(x)\n",
    "    x = tf.keras.layers.Dense(512,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.05)(x)\n",
    "    x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.05)(x)\n",
    "    x = tf.keras.layers.Dense(classes, activation=classifier_activation,name='output')(x)\n",
    "    model = tf.keras.Model(inputs=[input0,input1,input2],outputs=x)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.85)\n",
    "    model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaverickArchitecture2(input_shape=201,embeddingSize=1024,mmSize=51,classes=3,classifier_activation='softmax',**kwargs):\n",
    "    input0 = tf.keras.layers.Input(shape=(input_shape,mmSize),name='alt_cons')\n",
    "    input1 = tf.keras.layers.Input(shape=(input_shape,embeddingSize),name='alt_emb')\n",
    "    input2 = tf.keras.layers.Input(shape=12,name='non_seq_info')\n",
    "\n",
    "    # project input to an embedding size that is easier to work with\n",
    "    alt_cons = tf.keras.layers.experimental.EinsumDense('...x,xy->...y',output_shape=64,bias_axes='y')(input0)\n",
    "\n",
    "    posEnc_alt = official.nlp.keras_nlp.layers.PositionEmbedding(max_length=input_shape)(alt_cons)\n",
    "    alt_cons = tf.keras.layers.Masking()(alt_cons)\n",
    "    alt_cons = tf.keras.layers.Add()([alt_cons,posEnc_alt])\n",
    "    alt_cons = tf.keras.layers.LayerNormalization(axis=-1, epsilon=1e-12,dtype=tf.float32)(alt_cons)\n",
    "    alt_cons = tf.keras.layers.Dropout(0.05)(alt_cons)\n",
    "\n",
    "    transformer1 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer2 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer3 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer4 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer5 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer6 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    \n",
    "    alt_cons = transformer1(alt_cons)\n",
    "    alt_cons = transformer2(alt_cons)\n",
    "    alt_cons = transformer3(alt_cons)\n",
    "    alt_cons = transformer4(alt_cons)\n",
    "    alt_cons = transformer5(alt_cons)\n",
    "    alt_cons = transformer6(alt_cons)\n",
    "\n",
    "    first_token_tensor_alt = (tf.keras.layers.Lambda(lambda a: tf.squeeze(a[:, 100:101, :], axis=1))(alt_cons))\n",
    "    alt_cons = tf.keras.layers.Dense(units=64,activation='tanh')(first_token_tensor_alt)\n",
    "    alt_cons = tf.keras.layers.Dropout(0.05)(alt_cons)\n",
    "\n",
    "    sharedLSTM1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=False, dropout=0.5))\n",
    "\n",
    "    alt_emb=sharedLSTM1(input1)\n",
    "    alt_emb=tf.keras.layers.Dropout(0.2)(alt_emb)\n",
    "\n",
    "    structured = tf.keras.layers.Dense(64,activation='relu')(input2)\n",
    "    structured = tf.keras.layers.Dropout(0.05)(structured)\n",
    "    x = tf.keras.layers.concatenate([alt_cons,alt_emb,structured])\n",
    "    x = tf.keras.layers.Dropout(0.05)(x)\n",
    "    x = tf.keras.layers.Dense(512,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.05)(x)\n",
    "    x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.05)(x)\n",
    "    x = tf.keras.layers.Dense(classes, activation=classifier_activation,name='output')(x)\n",
    "    model = tf.keras.Model(inputs=[input0,input1,input2],outputs=x)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.85)\n",
    "    model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize=32\n",
    "inFile='input.annotated.txt'\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"prot_t5_xl_bfd\", do_lower_case=False,local_files_only=True)\n",
    "T5Model = TFT5EncoderModel.from_pretrained(\"prot_t5_xl_bfd\",local_files_only=True)\n",
    "\n",
    "# calculate medians and quantiles from training data\n",
    "trainingData=pandas.read_csv('trainingSet_v4.groomed_withExtraInfo2_corrected.txt',sep='\\t',low_memory=False)\n",
    "trainingData.loc[trainingData['GDI']>2000,'GDI']=2000\n",
    "trainingDataNonSeqInfo=trainingData[['controls_AF','controls_nhomalt','pLI','pNull','pRec','mis_z','lof_z','CCR','GDI','pext','RVIS_ExAC_0.05','gerp']].copy(deep=True)\n",
    "trainingDataNonSeqInfo.loc[trainingDataNonSeqInfo['controls_AF'].isna(),'controls_AF']=0\n",
    "trainingDataNonSeqInfo.loc[trainingDataNonSeqInfo['controls_nhomalt'].isna(),'controls_nhomalt']=0\n",
    "trainingDataNonSeqInfo.loc[trainingDataNonSeqInfo['controls_nhomalt']>10,'controls_nhomalt']=10\n",
    "trainingDataNonSeqMedians=trainingDataNonSeqInfo.median()\n",
    "trainingDataNonSeqInfo=trainingDataNonSeqInfo.fillna(trainingDataNonSeqMedians)\n",
    "trainingDataNonSeqInfo=np.asarray(trainingDataNonSeqInfo.to_numpy()).astype(np.float32)\n",
    "\n",
    "# scale columns by QT\n",
    "qt = QuantileTransformer(subsample=1e6, random_state=0, output_distribution='uniform')\n",
    "qt=qt.fit(trainingDataNonSeqInfo)\n",
    "trainingDataNonSeqInfo=qt.transform(trainingDataNonSeqInfo)\n",
    "\n",
    "# load the models\n",
    "model1 = MaverickArchitecture1()\n",
    "model1.load_weights('weights_TransformerNetDiff_model_1')\n",
    "model2 = MaverickArchitecture1()\n",
    "model2.load_weights('weights_TransformerNetDiff_classWeights_1_2_7_model_1')\n",
    "model3 = MaverickArchitecture1()\n",
    "model3.load_weights('weights_TransformerNetDiff_classWeights_1_2_7_model_2')\n",
    "model4 = MaverickArchitecture2()\n",
    "model4.load_weights('weights_T5_withBiLSTM_TransformerNet_altOnly_model_4')\n",
    "model5 = MaverickArchitecture2()\n",
    "model5.load_weights('weights_T5_withBiLSTM_TransformerNet_altOnly_model_5')\n",
    "model6 = MaverickArchitecture2()\n",
    "model6.load_weights('weights_T5_withBiLSTM_TransformerNet_altOnly_model_7')\n",
    "model7 = MaverickArchitecture2()\n",
    "model7.load_weights('weights_T5_withBiLSTM_TransformerNet_altOnly_classWeights_1_2_3_model_1')\n",
    "model8 = MaverickArchitecture2()\n",
    "model8.load_weights('weights_T5_withBiLSTM_TransformerNet_altOnly_classWeights_1_2_7_model_1')\n",
    "\n",
    "# prep the data\n",
    "inputData=pandas.read_csv(inFile,sep='\\t',low_memory=False)\n",
    "inputData.loc[inputData['GDI']>2000,'GDI']=2000\n",
    "inputDataNonSeqInfo=inputData[['controls_AF','controls_nhomalt','pLI','pNull','pRec','mis_z','lof_z','CCR','GDI','pext','RVIS_ExAC_0.05','gerp']].copy(deep=True)\n",
    "inputDataNonSeqInfo.loc[inputDataNonSeqInfo['controls_AF'].isna(),'controls_AF']=0\n",
    "inputDataNonSeqInfo.loc[inputDataNonSeqInfo['controls_nhomalt'].isna(),'controls_nhomalt']=0\n",
    "inputDataNonSeqInfo.loc[inputDataNonSeqInfo['controls_nhomalt']>10,'controls_nhomalt']=10\n",
    "inputDataNonSeqInfo=inputDataNonSeqInfo.fillna(trainingDataNonSeqMedians)\n",
    "inputDataNonSeqInfo=np.asarray(inputDataNonSeqInfo.to_numpy()).astype(np.float32)\n",
    "# scale columns by QT\n",
    "inputDataNonSeqInfo=qt.transform(inputDataNonSeqInfo)\n",
    "inputData.loc[:,['controls_AF','controls_nhomalt','pLI','pNull','pRec','mis_z','lof_z','CCR','GDI','pext','RVIS_ExAC_0.05','gerp']]=inputDataNonSeqInfo\n",
    "\n",
    "data_generator=DataGenerator(np.arange(len(inputData)),np.ones(len(inputData)),dataFrameIn=inputData,tokenizer=tokenizer,T5Model=T5Model,batch_size=batchSize,shuffle=False)\n",
    "\n",
    "# set up the output collectors\n",
    "models1Pred=''\n",
    "if genome=='GRCh37':\n",
    "    model1Preds=inputData.loc[:,['hg19_chr','hg19_pos(1-based)','ref','alt']]\n",
    "else:\n",
    "    model1Preds=inputData.loc[:,['hg38_chr','hg38_pos(1-based)','ref','alt']]\n",
    "model1Preds['BenignScore']=0\n",
    "model1Preds['DomScore']=0\n",
    "model1Preds['RecScore']=0\n",
    "model2Preds=model1Preds.copy(deep=True)\n",
    "model3Preds=model1Preds.copy(deep=True)\n",
    "model4Preds=model1Preds.copy(deep=True)\n",
    "model5Preds=model1Preds.copy(deep=True)\n",
    "model6Preds=model1Preds.copy(deep=True)\n",
    "model7Preds=model1Preds.copy(deep=True)\n",
    "model8Preds=model1Preds.copy(deep=True)\n",
    "\n",
    "\n",
    "# score the test data\n",
    "for batchNum in range(int(np.ceil(len(inputData)/batchSize))):\n",
    "    print('Starting batch number ' + str(batchNum), flush=True)\n",
    "    thisBatch=data_generator[batchNum]\n",
    "    thisBatchT5={'alt_cons':thisBatch[0]['alt_cons'],'alt_emb':thisBatch[0]['alt_emb'],'non_seq_info':thisBatch[0]['non_seq_info']}\n",
    "    thisBatchDiff={'mm_orig_seq':thisBatch[0]['mm_orig_seq'],'mm_alt_seq':thisBatch[0]['alt_cons'],'non_seq_info':thisBatch[0]['non_seq_info']}\n",
    "    model1Preds.loc[(batchNum*batchSize):((batchNum*batchSize)+len(thisBatch[1])-1),['BenignScore','DomScore','RecScore']]=model1.predict(thisBatchDiff,verbose=0)\n",
    "    model2Preds.loc[(batchNum*batchSize):((batchNum*batchSize)+len(thisBatch[1])-1),['BenignScore','DomScore','RecScore']]=model2.predict(thisBatchDiff,verbose=0)\n",
    "    model3Preds.loc[(batchNum*batchSize):((batchNum*batchSize)+len(thisBatch[1])-1),['BenignScore','DomScore','RecScore']]=model3.predict(thisBatchDiff,verbose=0)\n",
    "    model4Preds.loc[(batchNum*batchSize):((batchNum*batchSize)+len(thisBatch[1])-1),['BenignScore','DomScore','RecScore']]=model4.predict(thisBatchT5,verbose=0)\n",
    "    model5Preds.loc[(batchNum*batchSize):((batchNum*batchSize)+len(thisBatch[1])-1),['BenignScore','DomScore','RecScore']]=model5.predict(thisBatchT5,verbose=0)\n",
    "    model6Preds.loc[(batchNum*batchSize):((batchNum*batchSize)+len(thisBatch[1])-1),['BenignScore','DomScore','RecScore']]=model6.predict(thisBatchT5,verbose=0)\n",
    "    model7Preds.loc[(batchNum*batchSize):((batchNum*batchSize)+len(thisBatch[1])-1),['BenignScore','DomScore','RecScore']]=model7.predict(thisBatchT5,verbose=0)\n",
    "    model8Preds.loc[(batchNum*batchSize):((batchNum*batchSize)+len(thisBatch[1])-1),['BenignScore','DomScore','RecScore']]=model8.predict(thisBatchT5,verbose=0)\n",
    "\n",
    "# save individual model results to file\n",
    "model1Preds.to_csv(outBase + '_model1Predictions.txt',sep='\\t',index=False)\n",
    "model2Preds.to_csv(outBase + '_model2Predictions.txt',sep='\\t',index=False)\n",
    "model3Preds.to_csv(outBase + '_model3Predictions.txt',sep='\\t',index=False)\n",
    "model4Preds.to_csv(outBase + '_model4Predictions.txt',sep='\\t',index=False)\n",
    "model5Preds.to_csv(outBase + '_model5Predictions.txt',sep='\\t',index=False)\n",
    "model6Preds.to_csv(outBase + '_model6Predictions.txt',sep='\\t',index=False)\n",
    "model7Preds.to_csv(outBase + '_model7Predictions.txt',sep='\\t',index=False)\n",
    "model8Preds.to_csv(outBase + '_model8Predictions.txt',sep='\\t',index=False)\n",
    "\n",
    "# ensemble results together\n",
    "y_pred1=model1Preds.loc[:,['BenignScore','DomScore','RecScore']].to_numpy()\n",
    "y_pred2=model2Preds.loc[:,['BenignScore','DomScore','RecScore']].to_numpy()\n",
    "y_pred3=model3Preds.loc[:,['BenignScore','DomScore','RecScore']].to_numpy()\n",
    "y_pred4=model4Preds.loc[:,['BenignScore','DomScore','RecScore']].to_numpy()\n",
    "y_pred5=model5Preds.loc[:,['BenignScore','DomScore','RecScore']].to_numpy()\n",
    "y_pred6=model6Preds.loc[:,['BenignScore','DomScore','RecScore']].to_numpy()\n",
    "y_pred7=model7Preds.loc[:,['BenignScore','DomScore','RecScore']].to_numpy()\n",
    "y_pred8=model8Preds.loc[:,['BenignScore','DomScore','RecScore']].to_numpy()\n",
    "y_pred=np.mean([y_pred1,y_pred2,y_pred3,y_pred4,y_pred5,y_pred6,y_pred7,y_pred8],axis=0)\n",
    "model1Preds.loc[:,['BenignScore','DomScore','RecScore']]=y_pred\n",
    "model1Preds.to_csv(outBase + '_ensemblePredictions.txt',sep='\\t',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=model1Preds.copy(deep=True)\n",
    "if genome=='GRCh37':\n",
    "    sample['varID']=sample.loc[:,['hg19_chr','hg19_pos(1-based)','ref','alt']].apply(lambda row: '_'.join(row.values.astype(str)),axis=1)\n",
    "else:\n",
    "    sample['varID']=sample.loc[:,['hg38_chr','hg38_pos(1-based)','ref','alt']].apply(lambda row: '_'.join(row.values.astype(str)),axis=1)\n",
    "sample['TotalScore']=sample.loc[:,'Maverick_DomScore']\n",
    "sample.loc[sample['genotype']=='hom','TotalScore']=sample.loc[sample['genotype']=='hom','Maverick_RecScore']\n",
    "compHetPairs=pandas.DataFrame(columns=['site1_varID','site2_varID','geneID','geneName','site1_RecScore','site2_RecScore','TotalScore'])\n",
    "hets=sample.loc[sample['genotype']=='het',:].reset_index(drop=True)\n",
    "hetCallsOnSharedGenes=hets.loc[hets.duplicated(subset='geneID',keep=False),:]\n",
    "genesWithMultipleHets=hets.loc[hets.duplicated(subset='geneID',keep='first'),'geneID'].unique()\n",
    "for i in range(0,len(genesWithMultipleHets)):\n",
    "    thisGeneGroup=hetCallsOnSharedGenes.loc[hetCallsOnSharedGenes['geneID']==genesWithMultipleHets[i],:]\n",
    "    for j in range(0,len(thisGeneGroup)-1):\n",
    "        for k in range(j+1,len(thisGeneGroup)):\n",
    "            harmonicMean=scipy.stats.hmean([thisGeneGroup.loc[thisGeneGroup.index[j],'Maverick_RecScore'],thisGeneGroup.loc[thisGeneGroup.index[k],'Maverick_RecScore']])\n",
    "            compHetPairs=pandas.concat([compHetPairs,pandas.DataFrame({'site1_varID':thisGeneGroup.loc[thisGeneGroup.index[j],'varID'],\n",
    "                'site2_varID':thisGeneGroup.loc[thisGeneGroup.index[k],'varID'],\n",
    "                'geneID':thisGeneGroup.loc[thisGeneGroup.index[k],'geneID'],\n",
    "                'geneName':thisGeneGroup.loc[thisGeneGroup.index[k],'geneName'],\n",
    "                'site1_RecScore':thisGeneGroup.loc[thisGeneGroup.index[j],'Maverick_RecScore'],\n",
    "                'site2_RecScore':thisGeneGroup.loc[thisGeneGroup.index[k],'Maverick_RecScore'],\n",
    "                'TotalScore':harmonicMean},index=[0])],ignore_index=True)\n",
    "thisSampleFinalScores=pandas.concat([sample,compHetPairs],axis=0,sort=False,ignore_index=True)\n",
    "thisSampleFinalScores=thisSampleFinalScores.sort_values(by=\"TotalScore\",ascending=False)\n",
    "# tidy up\n",
    "if genome=='GRCh37':\n",
    "    thisSampleFinalScores=thisSampleFinalScores.loc[:,['varType','hg19_chr','hg19_pos(1-based)','ref','alt','genotype','geneName','geneID','Maverick_BenignScore','Maverick_DomScore','Maverick_RecScore','varID','site1_varID','site2_varID','site1_RecScore','site2_RecScore','TotalScore']]\n",
    "else:\n",
    "    thisSampleFinalScores=thisSampleFinalScores.loc[:,['varType','hg38_chr','hg38_pos(1-based)','ref','alt','genotype','geneName','geneID','Maverick_BenignScore','Maverick_DomScore','Maverick_RecScore','varID','site1_varID','site2_varID','site1_RecScore','site2_RecScore','TotalScore']]\n",
    "thisSampleFinalScores.to_csv(outBase + '.finalScores.txt',sep='\\t',header=True,index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
