{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget pandas numpy matplotlib scikit-learn scipy\n",
    "!pip install tensorflow==2.7\n",
    "!pip install tf-models-official==2.7\n",
    "!pip install transformers\n",
    "# download the resources\n",
    "!python -m wget https://zuchnerlab.s3.amazonaws.com/VariantPathogenicity/Maverick_resources.tar.gz\n",
    "!tar -zxvf Maverick_resources.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "import official.nlp\n",
    "import official.nlp.keras_nlp.layers\n",
    "import tensorflow_addons as tfa\n",
    "from transformers import TFT5EncoderModel, T5Tokenizer,T5Config\n",
    "import pandas\n",
    "pandas.options.mode.chained_assignment = None\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import scipy\n",
    "from scipy.stats import rankdata\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, list_IDs, labels, dataFrameIn, tokenizer, T5Model, batch_size=32, padding=100, n_channels_emb=1024, n_channels_mm=51, n_classes=3, returnStyle=1, shuffle=True):\n",
    "        self.padding = padding\n",
    "        self.dim = self.padding + self.padding + 1\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels_emb = n_channels_emb\n",
    "        self.n_channels_mm = n_channels_mm\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.dataFrameIn=dataFrameIn\n",
    "        self.tokenizer = tokenizer\n",
    "        self.T5Model = T5Model\n",
    "        self.returnStyle = returnStyle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        if (len(self.list_IDs) % self.batch_size) == 0:\n",
    "            return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "        else:\n",
    "            return int(np.ceil(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        if (((len(self.list_IDs) % self.batch_size) != 0) & (((index+1)*self.batch_size)>len(self.list_IDs))):\n",
    "            indexes = self.indexes[index*self.batch_size:]\n",
    "        else:\n",
    "            indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        thisBatchSize=len(list_IDs_temp)\n",
    "        altEmbeddings=np.zeros((thisBatchSize, self.dim, self.n_channels_emb))\n",
    "        mm_alt=np.zeros((thisBatchSize, self.dim, self.n_channels_mm))\n",
    "        mm_orig=np.zeros((thisBatchSize, self.dim, self.n_channels_mm))\n",
    "        nonSeq=np.zeros((thisBatchSize, 12))\n",
    "        y = np.empty((thisBatchSize), dtype=int)\n",
    "        AMINO_ACIDS = {'A':0,'C':1,'D':2,'E':3,'F':4,'G':5,'H':6,'I':7,'K':8,'L':9,'M':10,'N':11,'P':12,'Q':13,'R':14,'S':15,'T':16,'V':17,'W':18,'Y':19} \n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            if self.returnStyle==2:\n",
    "                # process Alt seq with T5 model to create embeddings\n",
    "                T5AltSeqTokens=[]\n",
    "                transcriptID=self.dataFrameIn.loc[ID,'TranscriptID']\n",
    "                changePos=self.dataFrameIn.loc[ID,'ChangePos']-1\n",
    "                if changePos<0:\n",
    "                    changePos=0\n",
    "                AltSeq=self.dataFrameIn.loc[ID,'AltSeq']\n",
    "                if AltSeq[-1]!=\"*\":\n",
    "                    AltSeq=AltSeq + \"*\"\n",
    "                seqLenAlt=len(AltSeq)-1\n",
    "                startPos=0\n",
    "                if changePos>self.padding:\n",
    "                    if (changePos+self.padding)<seqLenAlt:\n",
    "                        startPos=changePos-self.padding\n",
    "                    elif seqLenAlt>=self.dim:\n",
    "                        startPos=seqLenAlt-self.dim\n",
    "                endPos=changePos+self.padding\n",
    "                if changePos<self.padding:\n",
    "                    if self.dim<seqLenAlt:\n",
    "                        endPos=self.dim\n",
    "                    else:\n",
    "                        endPos=seqLenAlt\n",
    "                elif (changePos+self.padding)>=seqLenAlt:\n",
    "                    endPos=seqLenAlt\n",
    "                T5AltSeqTokens.append(\" \".join(AltSeq[startPos:endPos]))\n",
    "                # prep the WT seq too\n",
    "                WTSeq=self.dataFrameIn.loc[ID,'WildtypeSeq']\n",
    "                if WTSeq[-1]!=\"*\":\n",
    "                    WTSeq=WTSeq + \"*\"\n",
    "                seqLen=len(WTSeq)-1\n",
    "                startPos=0\n",
    "                if changePos>self.padding:\n",
    "                    if (changePos+self.padding)<seqLen:\n",
    "                        startPos=int(changePos-self.padding)\n",
    "                    elif seqLen>=self.dim:\n",
    "                        startPos=int(seqLen-self.dim)\n",
    "                endPos=int(changePos+self.padding)\n",
    "                if changePos<self.padding:\n",
    "                    if self.dim<seqLen:\n",
    "                        endPos=int(self.dim)\n",
    "                    else:\n",
    "                        endPos=int(seqLen)\n",
    "                elif (changePos+self.padding)>=seqLen:\n",
    "                    endPos=int(seqLen)\n",
    "                T5AltSeqTokens.append(\" \".join(WTSeq[startPos:endPos]))\n",
    "\n",
    "                # process the altSeq and wtSeq through the T5 tokenizer (for consistency with pre-computed data used for training)\n",
    "                allTokens=self.tokenizer.batch_encode_plus(T5AltSeqTokens,add_special_tokens=True, padding=True, return_tensors=\"tf\")\n",
    "                input_ids=allTokens['input_ids']\n",
    "                # but only process the altSeq through the T5 model\n",
    "                input_ids=tf.expand_dims(input_ids[0],0)\n",
    "                embeddings=self.T5Model(input_ids)\n",
    "                allEmbeddings=np.asarray(embeddings.last_hidden_state)\n",
    "                seq_len = (np.asarray(allTokens['attention_mask'])[0] == 1).sum()\n",
    "                seq_emb = allEmbeddings[0][1:seq_len-1]\n",
    "                altEmbeddings[i,:seq_emb.shape[0],:]=seq_emb\n",
    "\n",
    "            # collect MMSeqs WT info\n",
    "            tmp=np.load(\"HHMFiles/\" + transcriptID + \"_MMSeqsProfile.npz\",allow_pickle=True)\n",
    "            tmp=tmp['arr_0']\n",
    "            seqLen=tmp.shape[0]\n",
    "            startPos=changePos-self.padding\n",
    "            endPos=changePos+self.padding + 1\n",
    "            startOffset=0\n",
    "            endOffset=self.dim\n",
    "            if changePos<self.padding:\n",
    "                startPos=0\n",
    "                startOffset=self.padding-changePos\n",
    "            if (changePos + self.padding) >= seqLen:\n",
    "                endPos=seqLen\n",
    "                endOffset=self.padding + seqLen - changePos\n",
    "            mm_orig[i,startOffset:endOffset,:] = tmp[startPos:endPos,:]\n",
    "\n",
    "            # collect MMSeqs Alt info\n",
    "            # change the amino acid at 'ChangePos' and any after that if needed\n",
    "            varType=self.dataFrameIn.loc[ID,'varType']\n",
    "            WTSeq=self.dataFrameIn.loc[ID,'WildtypeSeq']\n",
    "            if varType=='nonsynonymous SNV':\n",
    "                if changePos==0:\n",
    "                    # then this transcript is ablated\n",
    "                    altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    altEncoded[:,0:20]=0\n",
    "                    altEncoded[:,50]=0\n",
    "                else:\n",
    "                    # change the single amino acid\n",
    "                    altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    altEncoded[changePos,AMINO_ACIDS[WTSeq[changePos]]]=0\n",
    "                    altEncoded[changePos,AMINO_ACIDS[AltSeq[changePos]]]=1\n",
    "            elif varType=='stopgain':\n",
    "                if changePos==0:\n",
    "                    # then this transcript is ablated\n",
    "                    altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    altEncoded[:,0:20]=0\n",
    "                    altEncoded[:,50]=0\n",
    "                elif seqLenAlt>seqLen:\n",
    "                    altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    for j in range(changePos,seqLen):\n",
    "                        altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    for j in range(seqLen,seqLenAlt):\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    altEncoded[seqLen:,50]=1\n",
    "                else:\n",
    "                    altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    altEncoded[changePos:,0:20]=0\n",
    "                    altEncoded[changePos:,50]=0\n",
    "            elif varType=='stoploss':\n",
    "                altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                altEncoded[:seqLen,:]=tmp\n",
    "                for j in range(seqLen,seqLenAlt):\n",
    "                    altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                altEncoded[seqLen:,50]=1\n",
    "            elif varType=='synonymous SNV':\n",
    "                # no change\n",
    "                altEncoded=tmp\n",
    "            elif ((varType=='frameshift deletion') | (varType=='frameshift insertion') | (varType=='frameshift substitution')):\n",
    "                if seqLen<seqLenAlt:\n",
    "                    altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    for j in range(changePos,seqLen):\n",
    "                        altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    for j in range(seqLen,seqLenAlt):\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    altEncoded[seqLen:,50]=1\n",
    "                elif seqLen>seqLenAlt:\n",
    "                    for j in range(changePos,seqLenAlt):\n",
    "                        tmp[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        tmp[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    for j in range(seqLenAlt,seqLen):\n",
    "                        tmp[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                    altEncoded=tmp\n",
    "                elif seqLen==seqLenAlt:\n",
    "                    for j in range(changePos,seqLen):\n",
    "                        tmp[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        tmp[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    altEncoded=tmp\n",
    "                else:\n",
    "                    print('Error: seqLen comparisons did not work')\n",
    "                    exit()\n",
    "            elif varType=='nonframeshift deletion':\n",
    "                # how many amino acids deleted?\n",
    "                altNucLen=0\n",
    "                if self.dataFrameIn.loc[ID,'alt']!='-':\n",
    "                    altNucLen=len(self.dataFrameIn.loc[ID,'alt'])\n",
    "                refNucLen=len(self.dataFrameIn.loc[ID,'ref'])\n",
    "                numAADel=int((refNucLen-altNucLen)/3)\n",
    "                if (seqLen-numAADel)==seqLenAlt:\n",
    "                    # non-frameshift deletion\n",
    "                    #altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                    #altEncoded[:changePos,:]=tmp[:changePos,:]\n",
    "                    #altEncoded[changePos:,:]=tmp[(changePos+numAADel):,:]\n",
    "                    for j in range(changePos,(changePos+numAADel)):\n",
    "                        tmp[j,:20]=0\n",
    "                    altEncoded=tmp\n",
    "                elif seqLen>=seqLenAlt:\n",
    "                    # early truncation\n",
    "                    altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    for j in range(changePos,seqLenAlt):\n",
    "                        altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    #for j in range(seqLenAlt,seqLen):\n",
    "                    #    altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                    altEncoded[seqLenAlt:,0:20]=0\n",
    "                    altEncoded[seqLenAlt:,50]=0\n",
    "                elif seqLen<seqLenAlt:\n",
    "                    # deletion causes stop-loss\n",
    "                    altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    for j in range(changePos,seqLen):\n",
    "                        altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    #for j in range(seqLen,seqLenAlt):\n",
    "                    #    altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    altEncoded[seqLen:,0:20]=0\n",
    "                    altEncoded[seqLen:,50]=0\n",
    "                else:\n",
    "                    print('Error: seqLen comparisons did not work for nonframeshift deletion')\n",
    "                    exit()\n",
    "            elif varType=='nonframeshift insertion':\n",
    "                # how many amino acids inserted?\n",
    "                refNucLen=0\n",
    "                if self.dataFrameIn.loc[ID,'ref']!='-':\n",
    "                    altNucLen=len(self.dataFrameIn.loc[ID,'ref'])\n",
    "                altNucLen=len(self.dataFrameIn.loc[ID,'alt'])\n",
    "                numAAIns=int((altNucLen-refNucLen)/3)\n",
    "                if (seqLen+numAAIns)==seqLenAlt:\n",
    "                    # non-frameshift insertion\n",
    "                    altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                    altEncoded[:changePos,:]=tmp[:changePos,:]\n",
    "                    altEncoded[(changePos+numAAIns):,:]=tmp[changePos:,:]\n",
    "                    for j in range(numAAIns):\n",
    "                        altEncoded[(changePos+j),AMINO_ACIDS[AltSeq[(changePos+j)]]]=1\n",
    "                    altEncoded[:,50]=1\n",
    "                elif seqLen<seqLenAlt:\n",
    "                    # stop loss\n",
    "                    altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    for j in range(changePos,seqLen):\n",
    "                        altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    for j in range(seqLen,seqLenAlt):\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    altEncoded[seqLen:,50]=1\n",
    "                elif seqLen>=seqLenAlt:\n",
    "                    # stop gain\n",
    "                    altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                    altEncoded[:seqLen,:]=tmp\n",
    "                    for j in range(changePos,seqLenAlt):\n",
    "                        altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                    altEncoded[seqLenAlt:,0:20]=0\n",
    "                    altEncoded[seqLenAlt:,50]=0\n",
    "                else:\n",
    "                    print('Error: seqLen comparisons did not work for nonframeshift insertion')\n",
    "                    exit()\n",
    "            elif varType=='nonframeshift substitution':\n",
    "                # is this an insertion or a deletion?\n",
    "                # note that there will not be any '-' symbols in these ref or alt fields because it is a substitution\n",
    "                refNucLen=len(self.dataFrameIn.loc[ID,'ref'])\n",
    "                altNucLen=len(self.dataFrameIn.loc[ID,'alt'])\n",
    "                if refNucLen>altNucLen:\n",
    "                    # deletion\n",
    "                    # does this cause an early truncation or non-frameshift deletion?\n",
    "                    if seqLen>seqLenAlt: \n",
    "                        numAADel=int((refNucLen-altNucLen)/3)\n",
    "                        if (seqLen-numAADel)==seqLenAlt:\n",
    "                            # non-frameshift deletion\n",
    "                            #altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                            #altEncoded[:changePos,:]=tmp[:changePos,:]\n",
    "                            #altEncoded[changePos:,:]=tmp[(changePos+numAADel):,:]\n",
    "                            for j in range(changePos,(changePos+numAADel)):\n",
    "                                tmp[j,:20]=0\n",
    "                            altEncoded=tmp\n",
    "                        else:\n",
    "                            # early truncation\n",
    "                            altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                            altEncoded[:seqLen,:]=tmp\n",
    "                            for j in range(changePos,seqLenAlt):\n",
    "                                altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                                altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                            #for j in range(seqLenAlt,seqLen):\n",
    "                            #    altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                            altEncoded[seqLenAlt:,0:20]=0\n",
    "                            altEncoded[seqLenAlt:,50]=0\n",
    "                    # does this cause a stop loss?\n",
    "                    elif seqLen<seqLenAlt:\n",
    "                        altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                        altEncoded[:seqLen,:]=tmp\n",
    "                        for j in range(changePos,seqLen):\n",
    "                            altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                        for j in range(seqLen,seqLenAlt):\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                        altEncoded[seqLen:,50]=1\n",
    "                    else: # not sure how this would happen\n",
    "                        altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                        altEncoded[:seqLen,:]=tmp\n",
    "                        for j in range(changePos,seqLen):\n",
    "                            altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                elif refNucLen<altNucLen:\n",
    "                    # insertion\n",
    "                    # does this cause a stop loss or non-frameshift insertion?\n",
    "                    if seqLen<seqLenAlt: \n",
    "                        numAAIns=int((altNucLen-refNucLen)/3)\n",
    "                        if (seqLen+numAAIns)==seqLenAlt:\n",
    "                            # non-frameshift insertion\n",
    "                            altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                            altEncoded[:changePos,:]=tmp[:changePos,:]\n",
    "                            altEncoded[(changePos+numAAIns):,:]=tmp[changePos:,:]\n",
    "                            for j in range(numAAIns):\n",
    "                                altEncoded[(changePos+j),AMINO_ACIDS[AltSeq[(changePos+j)]]]=1\n",
    "                            altEncoded[:,50]=1\n",
    "                        else:\n",
    "                            # stop loss\n",
    "                            altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                            altEncoded[:seqLen,:]=tmp\n",
    "                            for j in range(changePos,seqLen):\n",
    "                                altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                                altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                            for j in range(seqLen,seqLenAlt):\n",
    "                                altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                            altEncoded[:,50]=1\n",
    "                    # does this cause an early truncation?\n",
    "                    elif seqLen>seqLenAlt: \n",
    "                        altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                        altEncoded[:seqLen,:]=tmp\n",
    "                        for j in range(changePos,seqLenAlt):\n",
    "                            altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                        altEncoded[seqLenAlt:,0:20]=0\n",
    "                        #for j in range(seqLenAlt,seqLen):\n",
    "                        #    altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[seqLenAlt:,50]=0\n",
    "                    else: # not sure how this would happen\n",
    "                        altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                        altEncoded[:seqLen,:]=tmp\n",
    "                        for j in range(changePos,seqLen):\n",
    "                            altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                elif refNucLen==altNucLen:\n",
    "                    if seqLen==seqLenAlt:\n",
    "                        # synonymous or nonsynonymous change\n",
    "                        altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                        altEncoded[:seqLen,:]=tmp\n",
    "                        altEncoded[changePos,AMINO_ACIDS[WTSeq[changePos]]]=0\n",
    "                        altEncoded[changePos,AMINO_ACIDS[AltSeq[changePos]]]=1\n",
    "                    elif seqLen>seqLenAlt:\n",
    "                        # early truncation\n",
    "                        altEncoded=np.zeros((seqLen,self.n_channels_mm))\n",
    "                        altEncoded[:seqLen,:]=tmp\n",
    "                        for j in range(changePos,seqLenAlt):\n",
    "                            altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                        altEncoded[seqLenAlt:,0:20]=0\n",
    "                        #for j in range(seqLenAlt,seqLen):\n",
    "                        #    altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                        altEncoded[seqLenAlt:,50]=0\n",
    "                    elif seqLen<seqLenAlt:\n",
    "                        # stop loss\n",
    "                        altEncoded=np.zeros((seqLenAlt,self.n_channels_mm))\n",
    "                        altEncoded[:seqLen,:]=tmp\n",
    "                        for j in range(changePos,seqLen):\n",
    "                            altEncoded[j,AMINO_ACIDS[WTSeq[j]]]=0\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                        for j in range(seqLen,seqLenAlt):\n",
    "                            altEncoded[j,AMINO_ACIDS[AltSeq[j]]]=1\n",
    "                        altEncoded[seqLen:,50]=1\n",
    "                    else:\n",
    "                        print('non-frameshift substitution comparisons failed')\n",
    "                        exit()\n",
    "                else:\n",
    "                    print('Error: nonframeshift substitution nucleotide length comparison did not work')\n",
    "                    exit()\n",
    "            startPos=changePos-self.padding\n",
    "            endPos=changePos+self.padding+1\n",
    "            startOffset=0\n",
    "            endOffset=self.dim\n",
    "            if changePos<self.padding:\n",
    "                startPos=0\n",
    "                startOffset=self.padding-changePos\n",
    "            if (changePos + self.padding) >= seqLenAlt:\n",
    "                endPos=seqLenAlt\n",
    "                endOffset=self.padding + seqLenAlt - changePos\n",
    "            # exception to deal with start loss SNVs that create new frameshifted products longer than the original protein (when original was shorter than padding length)\n",
    "            if ((changePos==0) & (self.padding>=seqLen) & (seqLen<seqLenAlt) & (varType=='nonsynonymous SNV')):\n",
    "                endPos=seqLen\n",
    "                endOffset=self.padding + seqLen - changePos\n",
    "            elif ((changePos==0) & (varType=='stopgain')): # related exception for stopgains at position 0\n",
    "                if (seqLen+self.padding)<=self.dim:\n",
    "                    endPos=seqLen\n",
    "                    endOffset=self.padding + seqLen - changePos\n",
    "                else:\n",
    "                    endPos=self.padding+1\n",
    "                    endOffset=self.dim\n",
    "            mm_alt[i,startOffset:endOffset,:] = altEncoded[startPos:endPos,:]\n",
    "\n",
    "\n",
    "            # non-seq info\n",
    "            nonSeq[i] = self.dataFrameIn.loc[ID,['controls_AF','controls_nhomalt','pLI','pNull','pRec','mis_z','lof_z','CCR','GDI','pext','RVIS_ExAC_0.05','gerp']]\n",
    "            \n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        X={'mm_orig_seq':mm_orig,'mm_alt_seq':mm_alt,'non_seq_info':nonSeq}\n",
    "        if self.returnStyle==2:\n",
    "            X={'alt_cons':mm_alt,'alt_emb':altEmbeddings,'non_seq_info':nonSeq}\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaverickArchitecture1(input_shape=201,classes=3,classifier_activation='softmax',**kwargs):\n",
    "    input0 = tf.keras.layers.Input(shape=(input_shape,51),name='mm_orig_seq')\n",
    "    input1 = tf.keras.layers.Input(shape=(input_shape,51),name='mm_alt_seq')\n",
    "    input2 = tf.keras.layers.Input(shape=12,name='non_seq_info')\n",
    "\n",
    "    # project input to an embedding size that is easier to work with\n",
    "    x_orig = tf.keras.layers.experimental.EinsumDense('...x,xy->...y',output_shape=64,bias_axes='y')(input0)\n",
    "    x_alt = tf.keras.layers.experimental.EinsumDense('...x,xy->...y',output_shape=64,bias_axes='y')(input1)\n",
    "\n",
    "    posEnc_wt = official.nlp.keras_nlp.layers.PositionEmbedding(max_length=input_shape)(x_orig)\n",
    "    x_orig = tf.keras.layers.Masking()(x_orig)\n",
    "    x_orig = tf.keras.layers.Add()([x_orig,posEnc_wt])\n",
    "    x_orig = tf.keras.layers.LayerNormalization(axis=-1, epsilon=1e-12,dtype=tf.float32)(x_orig)\n",
    "    x_orig = tf.keras.layers.Dropout(0.05)(x_orig)\n",
    "\n",
    "    posEnc_alt = official.nlp.keras_nlp.layers.PositionEmbedding(max_length=input_shape)(x_alt)\n",
    "    x_alt = tf.keras.layers.Masking()(x_alt)\n",
    "    x_alt = tf.keras.layers.Add()([x_alt,posEnc_alt])\n",
    "    x_alt = tf.keras.layers.LayerNormalization(axis=-1, epsilon=1e-12,dtype=tf.float32)(x_alt)\n",
    "    x_alt = tf.keras.layers.Dropout(0.05)(x_alt)\n",
    "\n",
    "    transformer1 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer2 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer3 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer4 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer5 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer6 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    \n",
    "    x_orig = transformer1(x_orig)\n",
    "    x_orig = transformer2(x_orig)\n",
    "    x_orig = transformer3(x_orig)\n",
    "    x_orig = transformer4(x_orig)\n",
    "    x_orig = transformer5(x_orig)\n",
    "    x_orig = transformer6(x_orig)\n",
    "    \n",
    "    x_alt = transformer1(x_alt)\n",
    "    x_alt = transformer2(x_alt)\n",
    "    x_alt = transformer3(x_alt)\n",
    "    x_alt = transformer4(x_alt)\n",
    "    x_alt = transformer5(x_alt)\n",
    "    x_alt = transformer6(x_alt)\n",
    "\n",
    "    first_token_tensor_orig = (tf.keras.layers.Lambda(lambda a: tf.squeeze(a[:, 100:101, :], axis=1))(x_orig))\n",
    "    x_orig = tf.keras.layers.Dense(units=64,activation='tanh')(first_token_tensor_orig)\n",
    "    x_orig = tf.keras.layers.Dropout(0.05)(x_orig)\n",
    "\n",
    "    first_token_tensor_alt = (tf.keras.layers.Lambda(lambda a: tf.squeeze(a[:, 100:101, :], axis=1))(x_alt))\n",
    "    x_alt = tf.keras.layers.Dense(units=64,activation='tanh')(first_token_tensor_alt)\n",
    "    x_alt = tf.keras.layers.Dropout(0.05)(x_alt)\n",
    "\n",
    "    diff = tf.keras.layers.Subtract()([x_alt,x_orig])\n",
    "    combined = tf.keras.layers.concatenate([x_alt,diff])\n",
    "\n",
    "    input2Dense1 = tf.keras.layers.Dense(64,activation='relu')(input2)\n",
    "    input2Dense1 = tf.keras.layers.Dropout(0.05)(input2Dense1)\n",
    "    x = tf.keras.layers.concatenate([combined,input2Dense1])\n",
    "    x = tf.keras.layers.Dropout(0.05)(x)\n",
    "    x = tf.keras.layers.Dense(512,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.05)(x)\n",
    "    x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.05)(x)\n",
    "    x = tf.keras.layers.Dense(classes, activation=classifier_activation,name='output')(x)\n",
    "    model = tf.keras.Model(inputs=[input0,input1,input2],outputs=x)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.85)\n",
    "    model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaverickArchitecture2(input_shape=201,embeddingSize=1024,mmSize=51,classes=3,classifier_activation='softmax',**kwargs):\n",
    "    input0 = tf.keras.layers.Input(shape=(input_shape,mmSize),name='alt_cons')\n",
    "    input1 = tf.keras.layers.Input(shape=(input_shape,embeddingSize),name='alt_emb')\n",
    "    input2 = tf.keras.layers.Input(shape=12,name='non_seq_info')\n",
    "\n",
    "    # project input to an embedding size that is easier to work with\n",
    "    alt_cons = tf.keras.layers.experimental.EinsumDense('...x,xy->...y',output_shape=64,bias_axes='y')(input0)\n",
    "\n",
    "    posEnc_alt = official.nlp.keras_nlp.layers.PositionEmbedding(max_length=input_shape)(alt_cons)\n",
    "    alt_cons = tf.keras.layers.Masking()(alt_cons)\n",
    "    alt_cons = tf.keras.layers.Add()([alt_cons,posEnc_alt])\n",
    "    alt_cons = tf.keras.layers.LayerNormalization(axis=-1, epsilon=1e-12,dtype=tf.float32)(alt_cons)\n",
    "    alt_cons = tf.keras.layers.Dropout(0.05)(alt_cons)\n",
    "\n",
    "    transformer1 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer2 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer3 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer4 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer5 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    transformer6 = official.nlp.keras_nlp.layers.TransformerEncoderBlock(16,256,tf.keras.activations.relu,output_dropout=0.1,attention_dropout=0.1)\n",
    "    \n",
    "    alt_cons = transformer1(alt_cons)\n",
    "    alt_cons = transformer2(alt_cons)\n",
    "    alt_cons = transformer3(alt_cons)\n",
    "    alt_cons = transformer4(alt_cons)\n",
    "    alt_cons = transformer5(alt_cons)\n",
    "    alt_cons = transformer6(alt_cons)\n",
    "\n",
    "    first_token_tensor_alt = (tf.keras.layers.Lambda(lambda a: tf.squeeze(a[:, 100:101, :], axis=1))(alt_cons))\n",
    "    alt_cons = tf.keras.layers.Dense(units=64,activation='tanh')(first_token_tensor_alt)\n",
    "    alt_cons = tf.keras.layers.Dropout(0.05)(alt_cons)\n",
    "\n",
    "    sharedLSTM1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=False, dropout=0.5))\n",
    "\n",
    "    alt_emb=sharedLSTM1(input1)\n",
    "    alt_emb=tf.keras.layers.Dropout(0.2)(alt_emb)\n",
    "\n",
    "    structured = tf.keras.layers.Dense(64,activation='relu')(input2)\n",
    "    structured = tf.keras.layers.Dropout(0.05)(structured)\n",
    "    x = tf.keras.layers.concatenate([alt_cons,alt_emb,structured])\n",
    "    x = tf.keras.layers.Dropout(0.05)(x)\n",
    "    x = tf.keras.layers.Dense(512,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.05)(x)\n",
    "    x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.05)(x)\n",
    "    x = tf.keras.layers.Dense(classes, activation=classifier_activation,name='output')(x)\n",
    "    model = tf.keras.Model(inputs=[input0,input1,input2],outputs=x)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.85)\n",
    "    model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealer:\n",
    "    \n",
    "    def __init__(self, start, end, steps):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.steps = steps\n",
    "        self.n = 0\n",
    "        \n",
    "    def step(self):\n",
    "        self.n += 1\n",
    "        cos = np.cos(np.pi * (self.n / self.steps)) + 1\n",
    "        return self.end + (self.start - self.end) / 2. * cos\n",
    "\n",
    "\n",
    "class OneCycleScheduler(Callback):\n",
    "    \"\"\" `Callback` that schedules the learning rate on a 1cycle policy as per Leslie Smith's paper(https://arxiv.org/pdf/1803.09820.pdf).\n",
    "    If the model supports a momentum parameter, it will also be adapted by the schedule.\n",
    "    The implementation adopts additional improvements as per the fastai library: https://docs.fast.ai/callbacks.one_cycle.html, where\n",
    "    only two phases are used and the adaptation is done using cosine annealing.\n",
    "    In phase 1 the LR increases from `lr_max / div_factor` to `lr_max` and momentum decreases from `mom_max` to `mom_min`.\n",
    "    In the second phase the LR decreases from `lr_max` to `lr_max / (div_factor * 1e4)` and momemtum from `mom_max` to `mom_min`.\n",
    "    By default the phases are not of equal length, with the phase 1 percentage controlled by the parameter `phase_1_pct`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr_max, steps, mom_min=0.85, mom_max=0.95, phase_1_pct=0.3, div_factor=25.):\n",
    "        super(OneCycleScheduler, self).__init__()\n",
    "        lr_min = lr_max / div_factor\n",
    "        final_lr = lr_max / (div_factor * 1e4)\n",
    "        phase_1_steps = steps * phase_1_pct\n",
    "        phase_2_steps = steps - phase_1_steps\n",
    "        \n",
    "        self.phase_1_steps = phase_1_steps\n",
    "        self.phase_2_steps = phase_2_steps\n",
    "        self.phase = 0\n",
    "        self.step = 0\n",
    "        \n",
    "        self.phases = [[CosineAnnealer(lr_min, lr_max, phase_1_steps), CosineAnnealer(mom_max, mom_min, phase_1_steps)], \n",
    "                 [CosineAnnealer(lr_max, final_lr, phase_2_steps), CosineAnnealer(mom_min, mom_max, phase_2_steps)]]\n",
    "        \n",
    "        self.lrs = []\n",
    "        self.moms = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.phase = 0\n",
    "        self.step = 0\n",
    "\n",
    "        self.set_lr(self.lr_schedule().start)\n",
    "        self.set_momentum(self.mom_schedule().start)\n",
    "        \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        self.lrs.append(self.get_lr())\n",
    "        self.moms.append(self.get_momentum())\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.step += 1\n",
    "        if self.step >= self.phase_1_steps:\n",
    "            self.phase = 1\n",
    "            \n",
    "        self.set_lr(self.lr_schedule().step())\n",
    "        self.set_momentum(self.mom_schedule().step())\n",
    "        \n",
    "    def get_lr(self):\n",
    "        try:\n",
    "            return tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "        except AttributeError:\n",
    "            return None\n",
    "        \n",
    "    def get_momentum(self):\n",
    "        try:\n",
    "            return tf.keras.backend.get_value(self.model.optimizer.momentum)\n",
    "        except AttributeError:\n",
    "            return None\n",
    "        \n",
    "    def set_lr(self, lr):\n",
    "        try:\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "        except AttributeError:\n",
    "            pass # ignore\n",
    "        \n",
    "    def set_momentum(self, mom):\n",
    "        try:\n",
    "            tf.keras.backend.set_value(self.model.optimizer.momentum, mom)\n",
    "        except AttributeError:\n",
    "            pass # ignore\n",
    "\n",
    "    def lr_schedule(self):\n",
    "        return self.phases[self.phase][0]\n",
    "    \n",
    "    def mom_schedule(self):\n",
    "        return self.phases[self.phase][1]\n",
    "    \n",
    "    def plot(self):\n",
    "        ax = plt.subplot(1, 2, 1)\n",
    "        ax.plot(self.lrs)\n",
    "        ax.set_title('Learning Rate')\n",
    "        ax = plt.subplot(1, 2, 2)\n",
    "        ax.plot(self.moms)\n",
    "        ax.set_title('Momentum')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"prot_t5_xl_bfd\", do_lower_case=False,local_files_only=True)\n",
    "T5Model = TFT5EncoderModel.from_pretrained(\"prot_t5_xl_bfd\",local_files_only=True)\n",
    "\n",
    "# calculate medians and quantiles from training data\n",
    "trainingData=pandas.read_csv('trainingSet.txt',sep='\\t',low_memory=False)\n",
    "trainingData.loc[trainingData['GDI']>2000,'GDI']=2000\n",
    "trainingDataNonSeqInfo=trainingData[['controls_AF','controls_nhomalt','pLI','pNull','pRec','mis_z','lof_z','CCR','GDI','pext','RVIS_ExAC_0.05','gerp']].copy(deep=True)\n",
    "trainingDataNonSeqInfo.loc[trainingDataNonSeqInfo['controls_AF'].isna(),'controls_AF']=0\n",
    "trainingDataNonSeqInfo.loc[trainingDataNonSeqInfo['controls_nhomalt'].isna(),'controls_nhomalt']=0\n",
    "trainingDataNonSeqInfo.loc[trainingDataNonSeqInfo['controls_nhomalt']>10,'controls_nhomalt']=10\n",
    "trainingDataNonSeqMedians=trainingDataNonSeqInfo.median()\n",
    "trainingDataNonSeqInfo=trainingDataNonSeqInfo.fillna(trainingDataNonSeqMedians)\n",
    "trainingDataNonSeqInfo=np.asarray(trainingDataNonSeqInfo.to_numpy()).astype(np.float32)\n",
    "\n",
    "# scale columns by QT\n",
    "qt = QuantileTransformer(subsample=1e6, random_state=0, output_distribution='uniform')\n",
    "qt=qt.fit(trainingDataNonSeqInfo)\n",
    "trainingDataNonSeqInfo=qt.transform(trainingDataNonSeqInfo)\n",
    "\n",
    "# apply to validation set\n",
    "validationData=pandas.read_csv('validationSet.txt',sep='\\t',low_memory=False)\n",
    "validationData.loc[validationData['GDI']>2000,'GDI']=2000\n",
    "validationDataNonSeqInfo=validationData[['controls_AF','controls_nhomalt','pLI','pNull','pRec','mis_z','lof_z','CCR','GDI','pext','RVIS_ExAC_0.05','gerp']].copy(deep=True)\n",
    "validationDataNonSeqInfo.loc[validationDataNonSeqInfo['controls_AF'].isna(),'controls_AF']=0\n",
    "validationDataNonSeqInfo.loc[validationDataNonSeqInfo['controls_nhomalt'].isna(),'controls_nhomalt']=0\n",
    "validationDataNonSeqInfo.loc[validationDataNonSeqInfo['controls_nhomalt']>10,'controls_nhomalt']=10\n",
    "validationDataNonSeqInfo=validationDataNonSeqInfo.fillna(trainingDataNonSeqMedians)\n",
    "validationDataNonSeqInfo=np.asarray(validationDataNonSeqInfo.to_numpy()).astype(np.float32)\n",
    "validationDataNonSeqInfo=qt.transform(validationDataNonSeqInfo)\n",
    "validationData.loc[:,['controls_AF','controls_nhomalt','pLI','pNull','pRec','mis_z','lof_z','CCR','GDI','pext','RVIS_ExAC_0.05','gerp']]=validationDataNonSeqInfo\n",
    "\n",
    "# apply to known genes set\n",
    "knownData=pandas.read_csv('knownGenes.txt',sep='\\t',low_memory=False)\n",
    "knownData.loc[knownData['GDI']>2000,'GDI']=2000\n",
    "knownDataNonSeqInfo=knownData[['controls_AF','controls_nhomalt','pLI','pNull','pRec','mis_z','lof_z','CCR','GDI','pext','RVIS_ExAC_0.05','gerp']].copy(deep=True)\n",
    "knownDataNonSeqInfo.loc[knownDataNonSeqInfo['controls_AF'].isna(),'controls_AF']=0\n",
    "knownDataNonSeqInfo.loc[knownDataNonSeqInfo['controls_nhomalt'].isna(),'controls_nhomalt']=0\n",
    "knownDataNonSeqInfo.loc[knownDataNonSeqInfo['controls_nhomalt']>10,'controls_nhomalt']=10\n",
    "knownDataNonSeqInfo=knownDataNonSeqInfo.fillna(trainingDataNonSeqMedians)\n",
    "knownDataNonSeqInfo=np.asarray(knownDataNonSeqInfo.to_numpy()).astype(np.float32)\n",
    "knownDataNonSeqInfo=qt.transform(knownDataNonSeqInfo)\n",
    "knownData.loc[:,['controls_AF','controls_nhomalt','pLI','pNull','pRec','mis_z','lof_z','CCR','GDI','pext','RVIS_ExAC_0.05','gerp']]=knownDataNonSeqInfo\n",
    "\n",
    "# apply to novel genes set\n",
    "novelData=pandas.read_csv('novelGenes.txt',sep='\\t',low_memory=False)\n",
    "novelData.loc[novelData['GDI']>2000,'GDI']=2000\n",
    "novelDataNonSeqInfo=novelData[['controls_AF','controls_nhomalt','pLI','pNull','pRec','mis_z','lof_z','CCR','GDI','pext','RVIS_ExAC_0.05','gerp']].copy(deep=True)\n",
    "novelDataNonSeqInfo.loc[novelDataNonSeqInfo['controls_AF'].isna(),'controls_AF']=0\n",
    "novelDataNonSeqInfo.loc[novelDataNonSeqInfo['controls_nhomalt'].isna(),'controls_nhomalt']=0\n",
    "novelDataNonSeqInfo.loc[novelDataNonSeqInfo['controls_nhomalt']>10,'controls_nhomalt']=10\n",
    "novelDataNonSeqInfo=novelDataNonSeqInfo.fillna(trainingDataNonSeqMedians)\n",
    "novelDataNonSeqInfo=np.asarray(novelDataNonSeqInfo.to_numpy()).astype(np.float32)\n",
    "novelDataNonSeqInfo=qt.transform(novelDataNonSeqInfo)\n",
    "novelData.loc[:,['controls_AF','controls_nhomalt','pLI','pNull','pRec','mis_z','lof_z','CCR','GDI','pext','RVIS_ExAC_0.05','gerp']]=novelDataNonSeqInfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=trainingData.loc[:,'classLabel'].to_numpy()\n",
    "y_valid=validationData.loc[:,'classLabel'].to_numpy()\n",
    "y_known=knownData.loc[:,'classLabel'].to_numpy()\n",
    "y_novel=novelData.loc[:,'classLabel'].to_numpy()\n",
    "\n",
    "# one-hot encode the class label\n",
    "encoder=LabelEncoder()\n",
    "encoder.fit(trainingData.loc[:,'classLabel'])\n",
    "y_train_encoded=encoder.transform(trainingData.loc[:,'classLabel'])\n",
    "y_train_encoded=tf.keras.utils.to_categorical(y_train_encoded).astype(int)\n",
    "y_valid_encoded=encoder.transform(validationData.loc[:,'classLabel'])\n",
    "y_valid_encoded=tf.keras.utils.to_categorical(y_valid_encoded).astype(int)\n",
    "y_known_encoded=encoder.transform(knownData.loc[:,'classLabel'])\n",
    "y_known_encoded=tf.keras.utils.to_categorical(y_known_encoded).astype(int)\n",
    "y_novel_encoded=encoder.transform(novelData.loc[:,'classLabel'])\n",
    "y_novel_encoded=tf.keras.utils.to_categorical(y_novel_encoded).astype(int)\n",
    "\n",
    "# create data generators\n",
    "training_generator1=DataGenerator(np.arange(len(trainingData)),y_train,dataFrameIn=trainingData,batch_size=batchSize,returnStyle=1,shuffle=True)\n",
    "validation_generator1=DataGenerator(np.arange(len(validationData)),y_valid,dataFrameIn=validationData,batch_size=batchSize,returnStyle=1,shuffle=False)\n",
    "known_generator1=DataGenerator(np.arange(len(knownData)),y_known,dataFrameIn=knownData,batch_size=batchSize,returnStyle=1,shuffle=False)\n",
    "novel_generator1=DataGenerator(np.arange(len(novelData)),y_novel,dataFrameIn=novelData,batch_size=batchSize,returnStyle=1,shuffle=False)\n",
    "\n",
    "training_generator2=DataGenerator(np.arange(len(trainingData)),y_train,dataFrameIn=trainingData,batch_size=batchSize,returnStyle=2,shuffle=True)\n",
    "validation_generator2=DataGenerator(np.arange(len(validationData)),y_valid,dataFrameIn=validationData,batch_size=batchSize,returnStyle=2,shuffle=False)\n",
    "known_generator2=DataGenerator(np.arange(len(knownData)),y_known,dataFrameIn=knownData,batch_size=batchSize,returnStyle=2,shuffle=False)\n",
    "novel_generator2=DataGenerator(np.arange(len(novelData)),y_novel,dataFrameIn=novelData,batch_size=batchSize,returnStyle=,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Architecture 1, model 1\n",
    "batchSize=128\n",
    "numEpochs=20\n",
    "lr_schedule=OneCycleScheduler(0.1,19660) # this needs to be the number of steps you will train for (# of variants in training set * numEpochs / batchSize)\n",
    "modelWeightsName='weights_Architecture1_model1'\n",
    "callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath=modelWeightsName,save_weights_only=True,save_best_only=True,monitor='val_loss',verbose=1),lr_schedule]\n",
    "\n",
    "Arc1Model1 = MaverickArchitecture1()\n",
    "Arc1Model1.summary()\n",
    "Arc1Model1.fit(training_generator1,epochs=numEpochs,callbacks=callbacks,validation_data=validation_generator1)\n",
    "Arc1Model1.load_weights(modelWeightsName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Architecture 1, model 1\n",
    "y_valid_pred=Arc1Model1.predict(validation_generator1)\n",
    "print(\"Validation set performance\")\n",
    "print(classification_report(np.argmax(y_valid_encoded,axis=1).astype('int'), np.argmax(y_valid_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "validationData[['BenignScore','DomScore','RecScore']]=y_valid_pred\n",
    "validationData.to_csv('validationSet_withArchitecture1Model1Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_known_pred=Arc1Model1.predict(known_generator1)\n",
    "print(\"Known genes set performance\")\n",
    "print(classification_report(np.argmax(y_known_encoded,axis=1).astype('int'), np.argmax(y_known_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "knownData[['BenignScore','DomScore','RecScore']]=y_known_pred\n",
    "knownData.to_csv('knownGenes_withArchitecture1Model1Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_novel_pred=Arc1Model1.predict(novel_generator1)\n",
    "print(\"Novel genes set performance\")\n",
    "print(classification_report(np.argmax(y_novel_encoded,axis=1).astype('int'), np.argmax(y_novel_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "novelData[['BenignScore','DomScore','RecScore']]=y_novel_pred\n",
    "novelData.to_csv('novelGenes_withArchitecture1Model1Scores.txt',sep='\\t',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Architecture 1, model 2\n",
    "lr_schedule=OneCycleScheduler(0.1,19660)\n",
    "modelWeightsName='weights_Architecture1_model2'\n",
    "callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath=modelWeightsName,save_weights_only=True,save_best_only=True,monitor='val_loss',verbose=1),lr_schedule]\n",
    "\n",
    "Arc1Model2 = MaverickArchitecture1()\n",
    "Arc1Model2.summary()\n",
    "Arc1Model2.fit(training_generator1,epochs=numEpochs,callbacks=callbacks,validation_data=validation_generator1,class_weights={0:1,1:2,2:7})\n",
    "Arc1Model2.load_weights(modelWeightsName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Architecture 1, model 2\n",
    "y_valid_pred=Arc1Model2.predict(validation_generator1)\n",
    "print(\"Validation set performance\")\n",
    "print(classification_report(np.argmax(y_valid_encoded,axis=1).astype('int'), np.argmax(y_valid_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "validationData[['BenignScore','DomScore','RecScore']]=y_valid_pred\n",
    "validationData.to_csv('validationSet_withArchitecture1Model2Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_known_pred=Arc1Model2.predict(known_generator1)\n",
    "print(\"Known genes set performance\")\n",
    "print(classification_report(np.argmax(y_known_encoded,axis=1).astype('int'), np.argmax(y_known_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "knownData[['BenignScore','DomScore','RecScore']]=y_known_pred\n",
    "knownData.to_csv('knownGenes_withArchitecture1Model2Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_novel_pred=Arc1Model2.predict(novel_generator1)\n",
    "print(\"Novel genes set performance\")\n",
    "print(classification_report(np.argmax(y_novel_encoded,axis=1).astype('int'), np.argmax(y_novel_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "novelData[['BenignScore','DomScore','RecScore']]=y_novel_pred\n",
    "novelData.to_csv('novelGenes_withArchitecture1Model2Scores.txt',sep='\\t',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Architecture 1, model 3\n",
    "lr_schedule=OneCycleScheduler(0.1,19660)\n",
    "modelWeightsName='weights_Architecture1_model3'\n",
    "callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath=modelWeightsName,save_weights_only=True,save_best_only=True,monitor='val_loss',verbose=1),lr_schedule]\n",
    "\n",
    "Arc1Model3 = MaverickArchitecture1()\n",
    "Arc1Model3.summary()\n",
    "Arc1Model3.fit(training_generator1,epochs=numEpochs,callbacks=callbacks,validation_data=validation_generator1,class_weights={0:1,1:2,2:7})\n",
    "Arc1Model3.load_weights(modelWeightsName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Architecture 1, model 3\n",
    "y_valid_pred=Arc1Model3.predict(validation_generator1)\n",
    "print(\"Validation set performance\")\n",
    "print(classification_report(np.argmax(y_valid_encoded,axis=1).astype('int'), np.argmax(y_valid_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "validationData[['BenignScore','DomScore','RecScore']]=y_valid_pred\n",
    "validationData.to_csv('validationSet_withArchitecture1Model3Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_known_pred=Arc1Model3.predict(known_generator1)\n",
    "print(\"Known genes set performance\")\n",
    "print(classification_report(np.argmax(y_known_encoded,axis=1).astype('int'), np.argmax(y_known_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "knownData[['BenignScore','DomScore','RecScore']]=y_known_pred\n",
    "knownData.to_csv('knownGenes_withArchitecture1Model3Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_novel_pred=Arc1Model3.predict(novel_generator1)\n",
    "print(\"Novel genes set performance\")\n",
    "print(classification_report(np.argmax(y_novel_encoded,axis=1).astype('int'), np.argmax(y_novel_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "novelData[['BenignScore','DomScore','RecScore']]=y_novel_pred\n",
    "novelData.to_csv('novelGenes_withArchitecture1Model3Scores.txt',sep='\\t',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Architecture 2, model 1\n",
    "batchSize=16\n",
    "lr_schedule=OneCycleScheduler(0.1,157280)\n",
    "modelWeightsName='weights_Architecture2_model1'\n",
    "callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath=modelWeightsName,save_weights_only=True,save_best_only=True,monitor='val_loss',verbose=1),lr_schedule]\n",
    "\n",
    "Arc2Model1 = MaverickArchitecture2()\n",
    "Arc2Model1.summary()\n",
    "Arc2Model1.fit(training_generator2,epochs=numEpochs,callbacks=callbacks,validation_data=validation_generator2)\n",
    "Arc2Model1.load_weights(modelWeightsName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Architecture 2, model 1\n",
    "y_valid_pred=Arc2Model1.predict(validation_generator2)\n",
    "print(\"Validation set performance\")\n",
    "print(classification_report(np.argmax(y_valid_encoded,axis=1).astype('int'), np.argmax(y_valid_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "validationData[['BenignScore','DomScore','RecScore']]=y_valid_pred\n",
    "validationData.to_csv('validationSet_withArchitecture2Model1Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_known_pred=Arc2Model1.predict(known_generator2)\n",
    "print(\"Known genes set performance\")\n",
    "print(classification_report(np.argmax(y_known_encoded,axis=1).astype('int'), np.argmax(y_known_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "knownData[['BenignScore','DomScore','RecScore']]=y_known_pred\n",
    "knownData.to_csv('knownGenes_withArchitecture2Model1Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_novel_pred=Arc2Model1.predict(novel_generator2)\n",
    "print(\"Novel genes set performance\")\n",
    "print(classification_report(np.argmax(y_novel_encoded,axis=1).astype('int'), np.argmax(y_novel_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "novelData[['BenignScore','DomScore','RecScore']]=y_novel_pred\n",
    "novelData.to_csv('novelGenes_withArchitecture2Model1Scores.txt',sep='\\t',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Architecture 2, model 2\n",
    "lr_schedule=OneCycleScheduler(0.1,157280)\n",
    "modelWeightsName='weights_Architecture2_model2'\n",
    "callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath=modelWeightsName,save_weights_only=True,save_best_only=True,monitor='val_loss',verbose=1),lr_schedule]\n",
    "\n",
    "Arc2Model2 = MaverickArchitecture2()\n",
    "Arc2Model2.summary()\n",
    "Arc2Model2.fit(training_generator2,epochs=numEpochs,callbacks=callbacks,validation_data=validation_generator2)\n",
    "Arc2Model2.load_weights(modelWeightsName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Architecture 2, model 2\n",
    "y_valid_pred=Arc2Model2.predict(validation_generator2)\n",
    "print(\"Validation set performance\")\n",
    "print(classification_report(np.argmax(y_valid_encoded,axis=1).astype('int'), np.argmax(y_valid_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "validationData[['BenignScore','DomScore','RecScore']]=y_valid_pred\n",
    "validationData.to_csv('validationSet_withArchitecture2Model2Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_known_pred=Arc2Model2.predict(known_generator2)\n",
    "print(\"Known genes set performance\")\n",
    "print(classification_report(np.argmax(y_known_encoded,axis=1).astype('int'), np.argmax(y_known_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "knownData[['BenignScore','DomScore','RecScore']]=y_known_pred\n",
    "knownData.to_csv('knownGenes_withArchitecture2Model2Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_novel_pred=Arc2Model2.predict(novel_generator2)\n",
    "print(\"Novel genes set performance\")\n",
    "print(classification_report(np.argmax(y_novel_encoded,axis=1).astype('int'), np.argmax(y_novel_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "novelData[['BenignScore','DomScore','RecScore']]=y_novel_pred\n",
    "novelData.to_csv('novelGenes_withArchitecture2Model2Scores.txt',sep='\\t',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Architecture 2, model 3\n",
    "lr_schedule=OneCycleScheduler(0.1,157280)\n",
    "modelWeightsName='weights_Architecture2_model3'\n",
    "callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath=modelWeightsName,save_weights_only=True,save_best_only=True,monitor='val_loss',verbose=1),lr_schedule]\n",
    "\n",
    "Arc2Model3 = MaverickArchitecture2()\n",
    "Arc2Model3.summary()\n",
    "Arc2Model3.fit(training_generator2,epochs=numEpochs,callbacks=callbacks,validation_data=validation_generator2)\n",
    "Arc2Model3.load_weights(modelWeightsName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Architecture 2, model 3\n",
    "y_valid_pred=Arc2Model3.predict(validation_generator2)\n",
    "print(\"Validation set performance\")\n",
    "print(classification_report(np.argmax(y_valid_encoded,axis=1).astype('int'), np.argmax(y_valid_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "validationData[['BenignScore','DomScore','RecScore']]=y_valid_pred\n",
    "validationData.to_csv('validationSet_withArchitecture2Model3Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_known_pred=Arc2Model3.predict(known_generator2)\n",
    "print(\"Known genes set performance\")\n",
    "print(classification_report(np.argmax(y_known_encoded,axis=1).astype('int'), np.argmax(y_known_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "knownData[['BenignScore','DomScore','RecScore']]=y_known_pred\n",
    "knownData.to_csv('knownGenes_withArchitecture2Model3Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_novel_pred=Arc2Model3.predict(novel_generator2)\n",
    "print(\"Novel genes set performance\")\n",
    "print(classification_report(np.argmax(y_novel_encoded,axis=1).astype('int'), np.argmax(y_novel_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "novelData[['BenignScore','DomScore','RecScore']]=y_novel_pred\n",
    "novelData.to_csv('novelGenes_withArchitecture2Model3Scores.txt',sep='\\t',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Architecture 2, model 4\n",
    "lr_schedule=OneCycleScheduler(0.1,157280)\n",
    "modelWeightsName='weights_Architecture2_model4'\n",
    "callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath=modelWeightsName,save_weights_only=True,save_best_only=True,monitor='val_loss',verbose=1),lr_schedule]\n",
    "\n",
    "Arc2Model4 = MaverickArchitecture2()\n",
    "Arc2Model4.summary()\n",
    "Arc2Model4.fit(training_generator2,epochs=numEpochs,callbacks=callbacks,validation_data=validation_generator2,class_weights={0:1,1:2,2:3})\n",
    "Arc2Model4.load_weights(modelWeightsName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Architecture 2, model 4\n",
    "y_valid_pred=Arc2Model4.predict(validation_generator2)\n",
    "print(\"Validation set performance\")\n",
    "print(classification_report(np.argmax(y_valid_encoded,axis=1).astype('int'), np.argmax(y_valid_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "validationData[['BenignScore','DomScore','RecScore']]=y_valid_pred\n",
    "validationData.to_csv('validationSet_withArchitecture2Model4Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_known_pred=Arc2Model4.predict(known_generator2)\n",
    "print(\"Known genes set performance\")\n",
    "print(classification_report(np.argmax(y_known_encoded,axis=1).astype('int'), np.argmax(y_known_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "knownData[['BenignScore','DomScore','RecScore']]=y_known_pred\n",
    "knownData.to_csv('knownGenes_withArchitecture2Model4Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_novel_pred=Arc2Model4.predict(novel_generator2)\n",
    "print(\"Novel genes set performance\")\n",
    "print(classification_report(np.argmax(y_novel_encoded,axis=1).astype('int'), np.argmax(y_novel_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "novelData[['BenignScore','DomScore','RecScore']]=y_novel_pred\n",
    "novelData.to_csv('novelGenes_withArchitecture2Model4Scores.txt',sep='\\t',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Architecture 2, model 5\n",
    "lr_schedule=OneCycleScheduler(0.1,157280)\n",
    "modelWeightsName='weights_Architecture2_model5'\n",
    "callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath=modelWeightsName,save_weights_only=True,save_best_only=True,monitor='val_loss',verbose=1),lr_schedule]\n",
    "\n",
    "Arc2Model5 = MaverickArchitecture2()\n",
    "Arc2Model5.summary()\n",
    "Arc2Model5.fit(training_generator2,epochs=numEpochs,callbacks=callbacks,validation_data=validation_generator2,class_weights={0:1,1:2,2:7})\n",
    "Arc2Model5.load_weights(modelWeightsName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Architecture 2, model 5\n",
    "y_valid_pred=Arc2Model5predict(validation_generator2)\n",
    "print(\"Validation set performance\")\n",
    "print(classification_report(np.argmax(y_valid_encoded,axis=1).astype('int'), np.argmax(y_valid_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "validationData[['BenignScore','DomScore','RecScore']]=y_valid_pred\n",
    "validationData.to_csv('validationSet_withArchitecture2Model5Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_known_pred=Arc2Model5predict(known_generator2)\n",
    "print(\"Known genes set performance\")\n",
    "print(classification_report(np.argmax(y_known_encoded,axis=1).astype('int'), np.argmax(y_known_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "knownData[['BenignScore','DomScore','RecScore']]=y_known_pred\n",
    "knownData.to_csv('knownGenes_withArchitecture2Model5Scores.txt',sep='\\t',index=False)\n",
    "\n",
    "y_novel_pred=Arc2Model5predict(novel_generator2)\n",
    "print(\"Novel genes set performance\")\n",
    "print(classification_report(np.argmax(y_novel_encoded,axis=1).astype('int'), np.argmax(y_novel_pred,axis=1).astype('int'), target_names=['Benign','Dominant','Recessive'], digits=3))\n",
    "novelData[['BenignScore','DomScore','RecScore']]=y_novel_pred\n",
    "novelData.to_csv('novelGenes_withArchitecture2Model5Scores.txt',sep='\\t',index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
